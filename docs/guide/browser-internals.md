---
sidebar_position: 17.2
title: Руководство по работе современного браузера
description: Руководство по работе современного браузера
keywords: ['guide', 'browser', 'internals', 'v8', 'браузер', 'внутреннее устройство', 'руководство']
tags: ['guide', 'browser', 'internals', 'v8', 'браузер', 'внутреннее устройство', 'руководство']
---

> [Источник](https://addyo.substack.com/p/how-modern-browsers-work)

# Руководство по работе современного браузера

_Примечание: тем, кто стремится __досконально__ разобраться в том, как устроены браузеры, настоятельно рекомендую отличную книгу __"Browser Engineering"__ Павла Панчехи и Криса Харрелсона (доступна [здесь](https://browser.engineering/)). Эта серия статей — лишь общий обзор принципов работы браузеров._

Веб-разработчики нередко воспринимают браузер как __"черный ящик"__, который каким-то чудом превращает HTML, CSS и JS в интерактивные веб-приложения. На самом деле современный браузер — будь то Chrome (на базе [Chromium](https://www.chromium.org/chromium-projects/)), Firefox ([Gecko](https://firefox-source-docs.mozilla.org/overview/gecko.html)) или Safari ([WebKit](https://webkit.org/)) — представляет собой чрезвычайно сложное программное решение. Он управляет сетевыми запросами, разбирает (парсит) и выполняет код, рендерит графику с ускорением на графическом процессоре (GPU) и изолирует контент в отдельных процессах для обеспечения безопасности.

В этой серии статей мы подробно рассмотрим, __как устроены современные браузеры__, сделав акцент на архитектуре и внутреннем устройстве __Chromium__, но также отметим ключевые отличия в других браузерах. Мы рассмотрим весь цикл: от сетевого стека и конвейера парсинга до рендеринга с помощью [Blink](https://www.chromium.org/blink/), выполнения JS с помощью движка [V8](https://v8.dev/), загрузки модулей, многопроцессной архитектуры, песочниц безопасности и инструментов разработчика. Главная цель — дать понятное и доступное объяснение того, что происходит в браузере "под капотом".

<img src="https://habrastorage.org/webt/ev/vy/ig/evvyigyhuvvpt7yzd7y9bmha9uu.png" />
<br />

Начнем наше увлекательное путешествие.

## Сетевые операции и загрузка ресурсов

<img src="https://habrastorage.org/webt/9b/ac/ra/9bacrab9ku_xzxqv2sukfevgj-u.png" />
<br />

Загрузка любой страницы начинается с того, что сетевая подсистема браузера запрашивает необходимые ресурсы из Интернета. Когда мы вводим URL или кликаем по ссылке, пользовательский интерфейс браузера (работающий в так называемом [процессе браузера](https://www.chromium.org/developers/design-documents/multi-process-architecture/)) выполняет запрос на навигацию.

> __Процесс браузера__ — это главный управляющий процесс, который контролирует все остальные процессы и отвечает за работу пользовательского интерфейса. Все, что происходит вне конкретной вкладки, управляется именно им.

Процесс включает в себя следующие шаги:

* __Разбор URL и проверка безопасности:__ браузер анализирует введенный адрес, чтобы определить протокол (например, HTTP или HTTPS) и целевой домен. Он также решает, ввел ли пользователь поисковый запрос или это все же адрес сайта (как, например, в строке ввода Chrome). На этом этапе могут выполняться проверки безопасности — например, сверка с черными списками, чтобы защитить пользователя от фишинговых сайтов.
* __Поиск DNS:__ сетевая подсистема определяет IP-адрес домена (если он отсутствует в кэше). Для этого может потребоваться обращение к DNS-серверу. Современные браузеры могут использовать системные DNS-службы или DNS через HTTPS (DoH), если эти механизмы настроены, но в итоге цель одна — получить IP-адрес хоста.
* __Установка соединения:__ если с сервером нет активного подключения, браузер открывает новое. Для HTTPS-адресов это подразумевает выполнение TLS-рукопожатия — безопасного обмена ключами и проверки сертификатов. Поток сети браузера управляет протоколами (TCP, TLS и др.) самостоятельно, без участия пользователя.
* __Отправка HTTP-запроса:__ после установки соединения отправляется запрос (чаще всего HTTP GET) для получения ресурса. Современные браузеры используют HTTP/2 или HTTP/3, если сервер их поддерживает. Эти протоколы позволяют передавать несколько ресурсов по одному соединению, что ускоряет загрузку и устраняет ограничение старого HTTP/1.1 (около 6 параллельных запросов на хост). Например, с HTTP/2 HTML, CSS, JS и изображения могут загружаться одновременно по одному TCP/TLS-каналу, а HTTP/3 (работающий поверх QUIC/UDP) еще больше сокращает задержку при установке соединения.
* __Получение ответа:__ сервер возвращает HTTP-статус, заголовки и тело ответа (разметку HTML, данные JSON и т.д.). Браузер читает поток данных и при необходимости определяет тип содержимого, если заголовок `Content-Type` отсутствует или указан неверно. Например, если ответ выглядит как HTML, но не помечен как таковой, браузер все равно обработает его как HTML (в соответствии с гибкими веб-стандартами). На этом этапе также действуют механизмы безопасности: проверка `Content-Type`, блокировка подозрительных MIME-несоответствий и защита от межсайтовых утечек данных (например, CORB — Cross-Origin Read Blocking в Chrome). Кроме того, браузер сверяется с такими сервисами, как Safe Browsing, для блокировки вредоносных данных.
* __Перенаправления (редиректы) и последующие шаги:__ если ответ — это перенаправление (например, HTTP-статус 301 или 302 с заголовком `Location`), браузер следует по новому адресу (уведомив при этом UI-поток) и повторяет запрос. Только после получения итогового ответа с реальным содержимым браузер переходит к его обработке.

Все эти этапы выполняются внутри сетевой подсистемы (network stack), которая в Chromium работает в отдельном процессе — Network Service (часть инициативы по разделению сервисов, ["сервисификации"](https://www.chromium.org/servicification/)). Поток сети в основном процессе браузера координирует низкоуровневую работу с сокетами (sockets) через системные сетевые API. Такая архитектура важна для безопасности: рендерер, который выполняет код страницы, не имеет прямого доступа к сети — он лишь запрашивает нужные ресурсы у основного процесса.

### Спекулятивная загрузка и оптимизация ресурсов

Современные браузеры применяют сложные оптимизации производительности на этапе работы с сетью. Chrome, например, может выполнять предварительный DNS-запрос или открывать TCP-соединение, когда пользователь наводит курсор на ссылку или начинает вводить URL (механизмы "Predictor" и "Preconnect"). Это сокращает задержку при фактическом переходе на страницу. Также действует HTTP-кэширование: сетевой стек может обслуживать запросы из кэша браузера, если ресурс уже сохранен и актуален, избегая отправки запросов.

- [Полное руководство по HTTP-кэшированию](https://habr.com/ru/companies/timeweb/articles/954906/)

__Работа сканера предварительной загрузки:__ в Chromium реализован продвинутый сканер предзагрузки, который анализирует HTML-разметку до основного парсера. Когда основной HTML-парсер блокируется CSS или синхронным JS, сканер предзагрузки продолжает проверку разметки и выявляет ресурсы (изображения, скрипты, стили), которые можно загрузить параллельно. Этот механизм является ключевым для производительности современных браузеров и работает автоматически. Однако сканер не обнаруживает ресурсы, добавленные через JS, поэтому такие ресурсы загружаются последовательно.

- [JavaScript: заметка о сканере предварительной загрузки и пропуске невидимого контента](https://habr.com/ru/companies/timeweb/articles/671762/)

__[Ранние подсказки (HTTP 103)](https://developer.chrome.com/docs/web-platform/early-hints):__ позволяют серверам отправлять подсказки по ресурсам во время генерации основного ответа с помощью кода состояния HTTP 103. Это дает возможность заранее отправлять подсказки предварительного подключения (preconnect) и предзагрузки (preload) во время основной работы сервера, сокращая время отрисовки наибольшего содержимого страницы (Largest Contentful Paint, LCP) на несколько сотен миллисекунд. Ранние подсказки (Early Hints) доступны только для навигационных запросов и поддерживают директивы preconnect и preload, но не prefetch.

__[API правил спекулятивной загрузки](https://developer.chrome.com/docs/web-platform/implementing-speculation-rules):__ современный веб-стандарт, позволяющий задавать правила для динамической предзагрузки и предварительного рендеринга URL на основе поведения пользователя. В отличие от обычного prefetch, этот API может рендерить целые страницы с выполнением JS, обеспечивая почти мгновенную загрузку. Правила определяются в формате JSON внутри `<script>` или заголовков HTTP. В Chrome установлены ограничения для предотвращение их чрезмерного использования, с разными лимитами в зависимости от уровня важности.

__HTTP/2 и HTTP/3:__ большинство браузеров на базе Chromium и Firefox полностью поддерживают HTTP/2, а [HTTP/3](https://alexandrehtrb.github.io/posts/2024/03/http2-and-http3-explained/) (на основе QUIC) также широко доступен (в Chrome включен по умолчанию для поддерживающих сайтов). Эти протоколы ускоряют загрузку страниц за счет одновременной передачи нескольких ресурсов и сокращения расходов на установку соединений. Для разработчиков это означает, что теперь может отпасть необходимость в использовании спрайтов или разделении доменов — браузер эффективно загружает множество мелких файлов параллельно через одно соединение.

__Приоритет ресурсов:__ браузер распределяет ресурсы по приоритету. Обычно HTML и CSS имеют высокий приоритет (так как блокируют рендеринг), скрипты — средний или высокий приоритет при использовании атрибутов `defer/async`, а изображения — низкий. Сетевой стек Chromium распределяет приоритеты и может отменять или откладывать запросы, чтобы ускорить рендеринг. Разработчики могут влиять на приоритет с помощью [`<link rel=preload>`(https://web.dev/articles/preload-critical-assets)] и атрибута [Fetch Priority](https://web.dev/articles/fetch-priority).

К концу сетевого этапа браузер получает начальный HTML страницы (если это HTML-навигация). В этот момент процесс браузера выбирает процесс рендеринга (renderer process) для обработки контента. Chrome часто запускает новый рендерер параллельно с сетевым запросом (спекулятивно), чтобы он был готов к работе, когда данные поступят. Этот процесс изолирован и отвечает за парсинг и рендеринг страницы.

После полной или частичной загрузки ответа процесс браузера завершает навигацию: передает поток байтов рендереру для обработки. В этот момент обновляется адресная строка и индикатор безопасности (замок HTTPS и др.) для нового сайта. Дальше управление переходит к рендереру: парсинг HTML, загрузка подресурсов, выполнение скриптов и отрисовка страницы.

## Парсинг HTML, CSS и JavaScript

Когда процесс рендеринга получает HTML, его основной поток начинает разбор разметки в соответствии со спецификацией HTML. Результатом парсинга становится DOM (Document Object Model) — дерево объектов, представляющих структуру страницы. Парсинг выполняется поэтапно и может чередоваться с чтением данных из сети: браузеры анализируют HTML в потоковом режиме, поэтому построение DOM может начаться еще до полной загрузки HTML-файла.

<img src="https://habrastorage.org/webt/bb/bv/od/bbbvodafcfjbzjd_5aikqyiuz2k.png" />
<br />

__Парсинг HTML и построение DOM:__ парсинг HTML по стандарту определяется как устойчивый к ошибкам процесс, который всегда создает DOM, даже если разметка некорректна. Это значит, что если пропущен закрывающий тег `</p>` или нарушена вложенность тегов, парсер автоматически исправит или скорректирует дерево DOM. Например, разметка `<p>Hello <div>World</div>` в DOM будет интерпретирована так, что тег `<p>` закрывается перед `<div>`. Парсер создает элементы DOM и текстовые узлы для каждого тега и текста в HTML, размещая их в дереве в соответствии с вложенностью исходной разметки.

Важный момент: во время парсинга HTML парсер может наткнуться на ресурсы, которые нужно загрузить. Например, при встрече с `<link rel="stylesheet" href="...">` браузер отправит запрос за CSS-файлом (в сетевом потоке), а при встрече с `<img src="...">` — за изображением. Эти загрузки происходят параллельно с парсингом. Парсер продолжает работу, пока ресурсы загружаются, за одним важным исключением — скриптами.

__Обработка тегов `<script>`:__ если HTML-парсер встречает тег `<script>`, по умолчанию он приостанавливает парсинг и выполняет скрипт. Это необходимо, поскольку скрипты могут использовать `document.write()` или другие методы изменения DOM, которые могут влиять на структуру или содержимое страницы, которое еще загружается. Выполняя скрипт сразу, браузер сохраняет правильный порядок действий относительно HTML. Парсер передает скрипт движку JS для выполнения, и только после завершения скрипта (и применения всех изменений в DOM) парсинг HTML возобновляется. Именно из-за этого включение больших скриптов в `<head>` может замедлять рендеринг страницы — парсер не сможет продолжить работу, пока скрипт не будет полностью загружен и выполнен.

Разработчики могут изменить это поведение с помощью атрибутов [`defer` или `async`](https://web.dev/articles/efficiently-load-third-party-javascript) для тега `<script>` (или использовать современные ES6-модули).

`async`: скрипт загружается параллельно и выполняется сразу после загрузки, не останавливая HTML-парсинг. Парсер продолжает работу, а порядок выполнения скриптов относительно других асинхронных скриптов не гарантирован.
`defer`: скрипт загружается параллельно, но выполняется только после разбора всего HTML, при этом сохраняется порядок скриптов в документе.

В обоих случаях парсер продолжает работу, не дожидаясь выполнения скрипта, что, в целом, повышает производительность.

ES6-модули (`<script type="module">`) автоматически ведут себя как отложенные (`defer`) и могут использовать `import` для загрузки других модулей.

Использование этих техник позволяет браузеру продолжать построение DOM без длительных пауз, что ускоряет загрузку страниц.

__Парсинг CSS и построение CSSOM:__ помимо HTML, стили CSS также необходимо разобрать в структуру, с которой браузер может работать — обычно ее называют CSSOM (CSS Object Model). [CSSOM](https://web.dev/articles/critical-rendering-path/constructing-the-object-model) представляет собой набор всех стилей (правил, селекторов, свойств), применимых к документу.

CSS-парсер браузера считывает CSS-файлы или блоки `<style>` и превращает их в список правил CSS (с использованием различных оптимизаций, например, [фильтров Блума](https://habr.com/ru/companies/timeweb/articles/806383/), для ускорения применения стилей). Затем, по мере построения DOM (или когда DOM и CSSOM готовы), браузер вычисляет стили для каждого узла DOM. Этот этап обычно называют вычислением стилей (style resolution / style calculation).

Браузер объединяет DOM и CSSOM, чтобы определить, какие CSS-правила применяются к каждому элементу и каковы итоговые вычисленные стили (с учетом каскада, наследования и стилей по умолчанию). Результат обычно представляют как связывание каждого узла DOM с его вычисленными стилями — окончательными CSS-свойствами элемента (цвет, шрифт, размер и т.д.).

Следует отметить, что даже без CSS, заданного автором сайта, у каждого элемента есть стили по умолчанию браузера (user-agent stylesheet). Например, у `<h1>` есть стандартный размер шрифта и отступы почти во всех браузерах. Встроенные стили браузера применяются с наименьшим приоритетом, обеспечивая разумное отображение по умолчанию. Разработчики могут посмотреть вычисленные стили в DevTools (инструментах разработчика), чтобы увидеть, какие CSS-свойства в итоге применяются к элементу. Этап вычисления стилей учитывает все доступные правила — встроенные браузером, пользовательские и авторские — для окончательного оформления каждого элемента.

__Блокирующее рендеринг поведение:__ хоть парсинг HTML и может продолжаться без полной загрузки CSS, существует [блокирующая рендеринг зависимость](https://web.dev/learn/performance/understanding-the-critical-path): браузеры обычно ждут загрузки CSS (особенно в `<head>`), перед выполнением первого рендеринга. Это делается во избежание отображения неоформленного содержимого (flash of unstyled content). На практике, если `<script>` без атрибутов `async` или `defer` встречается перед `<link>` с CSS в HTML, его выполнение также будет ждать загрузки CSS, так как скрипты могут запрашивать информацию о стилях через DOM API.

Как правило, ссылки на таблицы стилей размещают в `<head>` (они блокируют рендеринг, но нужны на раннем этапе), а некритичные или крупные скрипты лучше помещать с `defer`/`async` или внизу страницы, чтобы они не задерживали парсинг DOM.

На этом этапе браузер имеет:

1. DOM, построенный из HTML.
2. CSSOM, полученный из разобранных CSS-правил.
3. Вычисленные стили для каждого узла DOM.

Эти компоненты формируют основу для следующего этапа — формирования макета страницы (layout).

Перед этим стоит подробнее рассмотреть, как движок JS (например, V8 в Chrome) выполняет код. Мы уже упоминали блокировку скриптов, но что происходит при их выполнении? Скрипты могут изменять DOM или CSSOM (например, через `document.createElement()` или установку стилей элементов). Браузеру может потребоваться повторно вычислять стили или заново формировать макет при таких изменениях, что при частом выполнении может негативно сказываться на производительности.

Начальное выполнение скриптов во время парсинга часто включает настройку обработчиков событий или манипуляции с DOM (например, шаблонизацию). После этого страница обычно полностью разобрана, и начинается этап формирования макета страницы и рендеринга.

## Стилизация и макетирование

На этом этапе процесс рендеринга браузера уже знает структуру DOM и вычисленные стили каждого элемента. Следующий вопрос: где на экране разместить все эти элементы и какого они размера?

Этим занимается layout (также называемый reflow или формирование (вычисление) макета (разметки)). На этом этапе браузер рассчитывает геометрию каждого элемента — его размеры и позицию — с учетом CSS-правил (поток документа, модель коробки (box model), flexbox, grid и др.) и иерархии DOM.

<img src="https://habrastorage.org/webt/gp/gk/oz/gpgkozgjija0xjkqly-vvrioesm.png" />
<br />

__Построение дерева разметки (layout tree):__ браузер проходит по дереву DOM и создает дерево разметки (также называемое render tree или frame tree). Оно похоже на DOM по структуре, но включает только визуальные элементы: например, теги `<script>` или `<meta>` не создают визуальных блоков. При этом, один HTML-элемент может быть представлен несколькими блоками, если он визуально разделен на части — например, текст, переносящийся на несколько строк. Каждый узел дерева разметки содержит вычисленные стили элемента, а также информацию о его содержимом (тексте, изображении) и свойствах, влияющих на размещение — ширина, высота, отступы и т.д.

На этом этапе браузер вычисляет точные координаты (x, y) и размеры (ширину и высоту) каждого блока. Для этого используются алгоритмы, определенные в спецификациях CSS, например: в обычном потоке документа блочные элементы располагаются сверху вниз и занимают всю доступную ширину, а строчные элементы выстраиваются в одну строку и переносятся на новую при нехватке места. Современные режимы компоновки — [flexbox](https://web.dev/learn/css/flexbox) и [grid](https://web.dev/learn/css/grid) применяют собственные алгоритмы.

При разметке текста браузер также учитывает метрики шрифта, чтобы правильно расставлять переносы строк, и обрабатывает такие особенности, как поля, внутренние отступы, границы и множество исключений — например, схлопывание отступов, плавающие элементы, абсолютно позиционированные элементы, исключенные из потока. Все это делает разметку одним из самых сложных этапов рендеринга — даже простая вертикальная разметка требует точных расчетов, зависящих от ширины контейнера, размера шрифта и других параметров. Именно поэтому в браузерных движках над реализацией разметки работают отдельные команды разработчиков, совершенствуя алгоритмы на протяжении многих лет.

Особенности дерева разметки:

* Элементы с `display: none` полностью исключаются из дерева разметки (они не создают никаких блоков). В отличие от них, элементы с `visibility: hidden` все же имеют блок в дереве (занимают место), но не отображаются при отрисовке.
* Псевдоэлементы, такие как `::before` или `::after`, имеющие содержимое, включаются в дерево разметки, так как создают визуальные блоки.
* Узлы дерева разметки содержат информацию о своей геометрии. Например, узел компоновки для элемента `<p>` знает свои координаты относительно области просмотра и размеры, а также содержит дочерние узлы для каждой строки или встроенного блока внутри него.

__Вычисление разметки__: формирование макета обычно является рекурсивным процессом. Начиная с корневого элемента `<html>`, браузер вычисляет размеры области просмотра для `<html>` и `<body>`, затем размещает внутри нее дочерние элементы и т.д. Размер многих элементов определяется размерами их родительских или дочерних элементов — например, контейнер может увеличиваться, чтобы вместить свои дочерние элементы, или дочерний элемент может занимать 50% ширины родителя. Алгоритм разметки иногда выполняет несколько проходов для обработки плавающих элементов или сложных взаимодействий, но в основном процесс идет сверху вниз с возможными откатами при необходимости.

К концу этого этапа известны позиции и размеры каждого элемента на странице. Концептуально страницу можно представить как набор блоков с текстом или изображениями внутри. Однако пока ничего не нарисовано на экране — это следующий этап, отрисовка (painting).

Важно отметить: разметка может потреблять значительные ресурсы и снижать производительность, особенно при повторных вычислениях. Если JS изменяет размер элемента или добавляет новый контент, это может привести к повторному вычислению разметки (relayout) для части или всей страницы. Разработчикам рекомендуется избегать многократного пересчета разметки (layout thrashing) — например, не получать данные о размерах элементов сразу после изменений DOM, чтобы не провоцировать синхронный пересчет разметки.

Браузер старается оптимизировать этот процесс, отслеживая, какие части дерева разметки помечены как "грязные", и пересчитывает только их. В худшем случае изменения на верхних уровнях DOM могут привести к пересчету разметки всей страницы. Поэтому ресурсоемкие операции со стилями и разметкой следует минимизировать для улучшения производительности.

__Итоги по стилям и разметке.__

Итак, из HTML и CSS браузер строит:

* DOM-дерево — структура и содержимое страницы
* CSSOM — разобранные CSS-правила
* вычисленные стили — результат применения правил CSS к каждому узлу DOM
* дерево разметки — DOM-дерево, содержащее только визуальные элементы, с рассчитанными размерами и положением каждого узла

Каждый этап строится на основе предыдущего. Если какой-либо этап меняется (например, скрипт изменяет DOM или свойство CSS), последующие этапы могут потребовать обновления. Например, если изменить CSS-класс у элемента, браузер может пересчитать стили для него (и для дочерних элементов, если меняется наследование), затем при необходимости пересчитать разметку, если изменения стиля влияют на геометрию (например, отображение или размеры), и после этого выполнить перерисовку.

Эта цепочка показывает, что макетирование и отрисовка зависят от актуальных стилей и предыдущих шагов. В разделе про DevTools мы рассмотрим, как отслеживать эти этапы и их длительность.

После формирования макета начинается следующий основной этап: отрисовка.

## Отрисовка, компоновка и рендеринг

Отрисовка — это этап, на котором структурированные данные о разметке превращаются в видимое изображение на экране. В традиционном подходе браузер обходил дерево разметки и выполнял команды отрисовки для каждого узла ("нарисовать фон, текст, изображение в этих координатах").

Современные браузеры по сути делают то же самое, но часто разбивают работу на несколько этапов и используют GPU для повышения эффективности.

<img src="https://habrastorage.org/webt/bh/3l/im/bh3lim5c0b2wv7c0pgv-t_bhb74.png" />
<br />

__Отрисовка/растеризация:__ в основном потоке рендерера, после этапа макетирования, Chrome создает записи для отрисовки (paint records) или список команд отображения (display list), обходя дерево компоновки. По сути, это список операций рисования с координатами, аналогичный плану художника:

* нарисовать прямоугольник в точке (x, y) с шириной W, высотой H и заливкой синим цветом
* нарисовать текст "Hello" в точке (x2, y2) со шрифтом XYZ
* нарисовать изображение… и т.д.

Список формируется в правильном порядке по `z-index` (чтобы элементы, перекрывающие друг друга, отображались корректно). Например, отрисовка элемента с более высоким `z-index` выполняется позже, поэтому он накладывается на контент с меньшим `z-index`. Браузер также учитывает контексты наложения (stacking contexts), прозрачность и другие особенности, чтобы получить нужный результат.

Раньше браузеры могли сразу рисовать каждый элемент на экране по мере обхода дерева разметки, но при изменениях на странице это было неэффективно — приходилось перерисовывать все целиком. Современные браузеры обычно сначала сохраняют команды рисования, а затем с помощью компоновки (compositing) собирают финальное изображение, особенно эффективно используя GPU.

__Слои и компоновка:__ компоновка — это оптимизация, при которой страница разбивается на несколько слоев, которые можно обрабатывать независимо. Например, позиционированный элемент с CSS-трансформацией или анимацией может получить собственный слой. Слои можно представить как отдельные "рабочие холсты" — браузер может нарисовать каждый слой отдельно, а затем композитор (compositor thread) объединяет их на экране, часто с использованием GPU.

В конвейере (pipeline) Chromium, после генерации записей для отрисовки, создается дерево слоев, которое определяет, какие элементы на каком слое находятся. Некоторые слои создаются автоматически (например, `<video>`, `<canvas>` или элементы с определенными CSS-свойствами), а разработчики могут давать браузеру подсказки через `will-change` или свойств, вроде `transform`. Слои полезны тем, что изменения позиции или прозрачности слоя можно "компоновать" — т.е. обновлять только этот слой без перерисовки всей страницы. Однако, слишком большое количество слоев увеличивает расход памяти и создает дополнительную нагрузку, поэтому браузеры используют их экономно.

После определения слоев основной поток Chrome передает работу композитору. Этот поток выполняется в процессе рендерера, но отдельно от основного потока (что позволяет ему работать даже при загрузке главного JS-потока — это обеспечивает плавность прокрутки и анимации). Задача композитора — взять слои, растеризовать их (превратить в пиксельные битовые карты (bitmaps)) и объединить в кадры для отображения (frames).

__Растеризация с помощью GPU:__ работа по растеризации может распределяться между потоками. В Chrome поток композитора разбивает слои на маленькие плитки (например, 256×256 или 512×512 пикселей, иногда больше при использовании GPU), а затем отправляет их нескольким рабочим потокам для параллельной растеризации. Каждый поток получает плитку — по сути, список команд рисования для этой области слоя — и создает растровое изображение (bitmap). Важно: библиотека Skia (графическая библиотека Chrome) может использовать центральный процессор (CPU) или GPU для растеризации. Обычно потоки используют CPU для отрисовки пикселей и загружают результат в память GPU. В Firefox WebRender делает это немного иначе.

После растеризации плитки хранятся в памяти GPU в виде текстур. Как только все необходимые плитки отрисованы, поток композитора фактически получает готовый набор текстурированных слоев. Композитор собирает композитный кадр — сообщение для браузерного процесса, включающее все плитки слоев и их позиции. Этот кадр отправляется обратно в процесс браузера через IPC (Inter-Process Communication - интерфейс межпроцессного взаимодействия), где отдельный GPU-процесс (в Chrome это отдельный процесс для работы с графикой) получает его и отображает на экране. Интерфейс самого браузера (например, панель вкладок) также рисуется с помощью композитных кадров, и все элементы объединяются на финальном этапе. GPU-процесс обрабатывает эти кадры с помощью GPU (через OpenGL, DirectX, Metal и т.д.), размещая каждую текстуру в нужной позиции на экране и применяя трансформации. В результате получается окончательное изображение, которое пользователь видит на экране.

Преимущество такой схемы особенно заметно при прокрутке или анимации. Например, при прокрутке страницы в основном изменяется лишь та часть большой текстуры страницы, которая видна в области просмотра. Поток композитора просто смещает позиции слоев и поручает GPU отрисовать новую видимую часть, без необходимости полной перерисовки в основном потоке.

Если анимация ограничивается только трансформацией (например, перемещением элемента, который находится на отдельном слое), поток композитора может обновлять позицию элемента для каждого кадра и формировать новые кадры без участия основного потока и без повторного расчета стилей и разметки. Поэтому для хорошей производительности рекомендуются анимации, затрагивающие только компоновку (изменение `transform` или `opacity`, которые не вызывают пересчет разметки) — они могут плавно работать с частотой 60 кадров в секунду (FPS) даже при загруженном основном потоке. В тоже время анимация таких свойств, как `height` или `background-color`, может потребовать повторного расчета разметки или перерисовки каждого кадра, что заметно замедляет процесс, если основной поток загружен.

Если описывать кратко, конвейер рендеринга в Chrome выглядит так: DOM → стиль → макет → отрисовка (создание списка отображения) → создание слоев → растеризация (tiles - плитки) → компоновка (GPU)

В Firefox процесс до этапа формирования списка отображения аналогичен, но WebRender передает этот список напрямую в GPU, где почти вся отрисовка выполняется с помощью шейдеров. Safari (WebKit) использует многопоточный композитор и GPU-рендеринг через CALayers на macOS. Все современные движки активно используют GPU для рендеринга и компоновки, что повышает частоту кадров и разгружает CPU.

Прежде чем двигаться дальше, разберем роль GPU подробнее. В Chromium процесс GPU выделен отдельно и отвечает за взаимодействие с графическим оборудованием. Он получает команды рисования (в основном высокого уровня, вроде "нарисуй эти текстуры в таких координатах") от всех потоков-композиторов и интерфейса браузера, а затем переводит их в реальные вызовы API графики. Такое разделение повышает надежность: если драйвер GPU даст сбой, это не "уронит" весь браузер — перезапускается только определенный процесс GPU. Кроме того, изоляция создает дополнительный уровень безопасности, ведь GPU обрабатывает потенциально небезопасный контент, вроде `<canvas>` или WebGL, где иногда возникают ошибки в драйверах — запуск в отдельном процессе снижает риски.

Результат компоновки в итоге отправляется на экран — в окно или контекст операционной системы, где запущен браузер. Для каждой анимации композитор старается формировать кадры с частотой около 60 FPS (примерно 16,7 мс на кадр) для плавного отображения. Если главный поток занят (например, выполнение JS заняло слишком много времени), композитор может пропустить кадры или не успеть их обновить, что приводит к подтормаживанию анимации. Инструменты разработчика позволяют увидеть такие пропущенные кадры на временной шкале производительности. Методы, вроде `requestAnimationFrame()`, помогают синхронизировать обновления JS с кадрами анимации, обеспечивая более плавное воспроизведение.

Подводя итог, можно сказать, что движок рендеринга браузера последовательно преобразует содержимое страницы и ее стили в геометрию (разметку) и набор инструкций для рисования. После этого, с помощью слоев и GPU-компоновки, он эффективно превращает все это в пиксели, которые мы видим на экране. Благодаря этой сложной, но отлаженной цепочке процессов современные браузеры способны отображать насыщенную графику и плавные анимации с высокой частотой кадров.

Далее мы заглянем внутрь движка JS, чтобы понять, как браузер выполняет скрипты — ту часть, которая до этого была для нас "черным ящиком".

## Внутри движка JavaScript (V8)

JavaScript отвечает за интерактивное поведение веб-страниц. В браузерах на базе Chromium его выполнение обеспечивает движок V8, который также обрабатывает WebAssembly. Понимание принципов работы V8 помогает разработчикам писать более эффективный код.

Полное изучение движка потребовало бы отдельной книги, поэтому здесь мы рассмотрим только основные этапы его работы: парсинг и компиляцию кода, его выполнение и управление памятью (включая сборку мусора). Также мы затронем современные механизмы V8 — многоуровневую JIT-компиляцию и поддержку модулей ES.

<img src="https://habrastorage.org/webt/zc/fh/md/zcfhmdu5iptm3dqex3vkmqtzaty.png" />
<br />

### Современный конвейер парсинга и компиляции в V8

<img src="https://habrastorage.org/webt/z0/vx/t9/z0vxt9wrv87ofvbnaxczepxy6c0.png" />
<br />

__Фоновая компиляция:__ начиная с Chrome 66, V8 выполняет компиляцию исходного кода JS в фоновом потоке, что сокращает время, затрачиваемое на компиляцию в основном потоке, примерно на 5–20% на большинстве сайтов. С версии Chrome 41 движок поддерживает фоновый парсинг JS-файлов через API V8 StreamedSource. Это означает, что V8 может начинать парсинг кода сразу после загрузки первых байтов из сети и продолжать его параллельно с получением оставшейся части файла. Практически вся компиляция скриптов выполняется в фоновых потоках — в основном потоке остаются только короткие этапы: внутренняя обработка абстрактного синтаксического дерева (AST) и финализация (finalizing) байт-кода (bytecode) непосредственно перед выполнением скрипта. На данный момент верхнеуровневый код (top-level script) и немедленно вызываемые функции (IIFE) компилируются в фоновых потоках, а вложенные функции по-прежнему компилируются лениво (lazy compilation) — т.е. только при первом выполнении, уже в основном потоке.

__Парсинг и байт-код:__ когда браузер встречает тег `<script>` (во время парсинга HTML или при последующей загрузке), движок V8 сначала разбирает исходный код JS. Результатом этого этапа является формирование AST — структурированного представления кода.

Предварительный парсер (preparser) — это облегченная версия парсера, выполняющая минимально необходимую работу по "валидации" функций. Он проверяет, что функции синтаксически корректны, и подготавливает данные, необходимые для правильной компиляции внешних функций. Когда предварительно разобранная функция вызывается впервые, она полностью парсится и компилируется по требованию.

Вместо того, чтобы интерпретировать код напрямую из AST, V8 использует интерпретатор байт-кода Ignition (введенный в 2016 году). Ignition компилирует JS в компактный байт-код — последовательность инструкций для виртуальной машины. Такая начальная компиляция выполняется очень быстро, а байт-код довольно низкоуровневый (Ignition — регистровая виртуальная машина). Главная цель — начать выполнение кода как можно быстрее, с минимальными затратами времени на подготовку, что особенно важно для скорости загрузки страниц.

__Процесс интернализации AST:__ интернализация (internalization) AST включает выделение в куче V8 литералов — строк, чисел и шаблонов объектных литералов — для последующего использования сгенерированным байт-кодом. Чтобы реализовать фоновую компиляцию, этот процесс был перенесен на более поздний этап — после компиляции в байт-код. Это потребовало изменений, позволяющих обращаться к "сырым" значениям литералов, встроенным в AST, вместо уже интернализированных объектов в куче.

__Явные подсказки компиляции:__ недавно V8 ввел новую возможность под названием [Explicit Compile Hints](https://v8.dev/blog/explicit-compile-hints), которая позволяет разработчикам явно указывать, что код нужно разобрать и скомпилировать сразу при загрузке — с помощью ранней компиляции. Файлы с такой подсказкой компилируются в фоновом потоке, тогда как отложенная компиляция выполняется в основном потоке. Эксперименты с популярными веб-страницами показали улучшение производительности в 17 из 20 случаев, при среднем сокращении времени парсинга и компиляции на 630 мс. Разработчики могут добавлять явные подсказки в специальные комментарии в JS-файлы, чтобы включить раннюю компиляцию критически важных участков кода в фоновом режиме.

__Оптимизации сканера и парсера:__ сканер V8 был значительно улучшен, что дало прирост производительности во всех направлениях: сканирование одного токена ускорилось примерно в 1,4 раза, сканирование строк — в 1,3 раза, сканирование многострочных комментариев — в 2,1 раза, а сканирование идентификаторов — в 1,2–1,5 раза в зависимости от длины идентификатора.

Когда скрипт выполняется, Ignition интерпретирует байт-код и запускает программу. Интерпретация обычно медленнее выполнения оптимизированного машинного кода, но она позволяет движку сразу начать выполнение и одновременно собирать профилирующую информацию о поведении кода. По мере выполнения V8 собирает данные о том, как используется код: типы переменных, какие функции вызываются чаще всего и т.д. Эта информация затем используется для ускорения выполнения кода на последующих этапах.

### Уровни JIT-компиляции

V8 не ограничивается интерпретацией. Он использует несколько уровней компиляторов Just-In-Time (JIT - компиляция кода во время выполнения), чтобы ускорять часто выполняемый код. Идея в том, чтобы тратить больше ресурсов на оптимизацию часто выполняемого кода, делая его быстрее, и при этом не тратить время на код, который выполняется редко.

1. __Ignition__ — интерпретирует байт-код.
2. __Sparkplug:__ базовый JIT-компилятор V8, запущенный примерно в 2021 году. Sparkplug берет байт-код и быстро компилирует его в машинный код без глубоких оптимизаций. Полученный нативный код работает быстрее интерпретации, но Sparkplug не проводит сложного анализа — он почти так же быстр при запуске, как интерпретатор, но дает код, который работает немного быстрее.
3. __Maglev:__ в 2023 году V8 представил Maglev — компилятор среднего уровня с оптимизациями. Maglev генерирует код примерно в 20 раз медленнее Sparkplug, но в 10–100 раз быстрее TurboFan, эффективно заполняя промежуток для функций, которые вызываются часто, но недостаточно часто для TurboFan. Maglev применяется к функциям с умеренной активностью — достаточно часто вызываемым, чтобы требовать оптимизации, но не настолько, чтобы задействовать TurboFan, или в случаях, когда компиляция через TurboFan была бы слишком затратной. Начиная с Chrome M117, Maglev способен обрабатывать многие такие случаи, ускоряя запуск веб-приложений для участков кода со средней активностью, заполняя промежуток между базовым JIT и высокоэффективным компилятором.
4. __TurboFan:__ когда функции или циклы выполняются много раз, V8 включает свой самый мощный оптимизирующий компилятор. TurboFan использует собранную информацию о типах для генерации высоко оптимизированного машинного кода, применяя продвинутые оптимизации (встраивание функций, устранение проверок границ и т.д.). Такой код может работать значительно быстрее, если выполнены предполагаемые условия.

Итак, у V8 теперь фактически четыре уровня исполнения: интерпретатор Ignition, базовый JIT Sparkplug, оптимизирующий JIT Maglev и оптимизирующий JIT TurboFan. Это похоже на многоуровневую JIT-компиляцию в Java HotSpot VM (C1 и C2). Движок динамически решает, какие функции и когда оптимизировать, исходя из профиля выполнения. Если функция внезапно вызывается миллион раз, она, скорее всего, будет оптимизирована TurboFan для максимальной скорости.

Intel также разработала [Profile-Guided Tiering](https://community.intel.com/t5/Blogs/Tech-Innovation/Client/Profile-Guided-Tiering-in-the-V8-JavaScript-Engine/post/1679340), что повышает эффективность V8, обеспечивая примерно 5% прироста в бенчмарках Speedometer 3. В последних версиях V8 реализована оптимизация статических корней, позволяющая на этапе компиляции точно определять адреса памяти для часто используемых объектов, что заметно ускоряет к ним доступ.

Одна из проблем JIT-оптимизации заключается в том, что JS — язык с динамической типизацией. V8 может оптимизировать код, исходя из определенных предположений (например, что переменная всегда хранит целое число). Если позднее эти предположения нарушаются (например, переменная становится строкой), оптимизированный код становится недействительным. Тогда V8 выполняет деоптимизацию: возвращается к менее оптимизированной версии или генерирует код заново с учетом новых предположений. Этот механизм опирается на встроенные кэши (inline caches) и обратную связь по типам, чтобы быстро адаптироваться. Из-за деоптимизации пиковая производительность может снижаться, если типы данных непредсказуемы, однако V8 обычно эффективно работает с типичными случаями, например, когда функции получают объекты одного и того же типа последовательно.

### Сброс байт-кода и управление памятью

V8 поддерживает сброс байт-кода: если функция долго не вызывается и проходит несколько циклов сборки мусора, ее байт-код выгружается из памяти. При следующем обращении к функции парсер использует сохраненные данные, чтобы быстрее восстановить байт-код. Этот механизм помогает эффективно управлять памятью, но в редких случаях может приводить к небольшим несоответствиям при парсинге кода.

__Управление памятью (сборка мусора):__ V8 автоматически управляет памятью JS-объектов с помощью сборщика мусора. Со временем он эволюционировал в Orinoco GC — современный, поколенческий и инкрементальный сборщик мусора, который выполняется одновременно с основным потоком. Основные моменты:

* __Поколенческая сборка:__ V8 разделяет объекты по их "возрасту". Новые объекты помещаются в молодое поколение (или nursery), где они часто очищаются с помощью быстрого алгоритма scavenging — "живые" объекты копируются в новое пространство, а остальная память освобождается. Объекты, которые пережили несколько таких циклов, перемещаются в старшее поколение.
* __Mark-and-sweep с уплотнением:__ для старшего поколения V8 применяет сборщик Mark-and-sweep с уплотнением. Это значит, что периодически выполнение JS останавливается (stop-the-world), собираются все доступные объекты (начиная от корней, например, глобального объекта), а память, занимаемая неиспользуемыми объектами, освобождается. Дополнительно может выполняться уплотнение памяти (перемещение объектов для уменьшения фрагментации памяти). Однако в Orinoco большая часть этапа маркировки (mark) выполняется конкурентно — в фоновых потоках, пока JS продолжает работать, что позволяет минимизировать паузы.
* __Инкрементальная сборка:__ V8 выполняет сборку мусора по частям, а не одной большой паузой. Такой подход распределяет работу и помогает избежать "подвисаний". Например, часть маркировки может выполняться между выполнениями скриптов, используя периоды простоя (idle time).
* __Параллельная сборка:__ на многопроцессорных машинах V8 может выполнять части сборки (маркировку или очистку (sweep)) параллельно в нескольких потоках.

В итоге команда V8 за годы работы существенно сократила паузы сборки мусора, сделав ее практически незаметной даже в больших приложениях. Небольшие сборки (для новых объектов, "Young space") обычно проходят очень быстро. Основные сборки (для старого поколения, "Old space") встречаются реже и теперь выполняются преимущественно параллельно с работой скриптов. В диспетчере задач Chrome или в панели "Память" DevTools можно увидеть кучу V8, разделенную на "Young space" и "Old space", что отражает поколенческую организацию памяти.

Для разработчиков это означает, что управление памятью вручную не требуется, но все же стоит учитывать некоторые моменты: например, не создавать слишком много короткоживущих объектов в горячих циклах (хотя V8 эффективно с ними справляется) и помнить, что удержание больших структур данных будет сохранять их в памяти. Инструменты, вроде DevTools, позволяют принудительно запустить сборку мусора или записать профили памяти, чтобы понять, что занимает больше всего места.

__V8 и веб-API:__ важно понимать, что V8 отвечает только за сам язык JS и его среду выполнения (интерпретацию кода, стандартные объекты JS и т.д.). Многие "браузерные API" — например, методы DOM, `alert()`, сетевые запросы через XHR или `fetch` — не входят в состав V8. Их предоставляет браузер, и они становятся доступными в JS через специальные привязки (bindings). Например, при вызове `document.querySelector()` движок через привязку обращается к реализации DOM на C++. V8 обрабатывает этот вызов и возвращает результат, а граница между JS и C++ оптимизирована для скорости работы (в Chrome используется специальный язык описания интерфейсов (IDL) для генерации эффективных привязок).

Мы рассмотрели, как браузер загружает ресурсы, парсит HTML и CSS, вычисляет разметку, рендерит ее с помощью GPU и выполняет JS. Теперь у нас есть целостное представление о процессе загрузки и отображения страницы. Но есть и другие аспекты для изучения: как работают ES-модули (у них свой механизм загрузки), как устроена многопроцессная архитектура браузера и как реализованы функции безопасности, такие как песочница и изоляция сайтов.

## Загрузка модулей и карта импортов

[Модули JavaScript](https://v8.dev/features/modules) (ES6-модули) используют иной подход к загрузке и выполнению кода, по сравнению с обычными тегами `<script>`. Вместо одного большого файла, создающего глобальные переменные, модули представляют собой отдельные файлы, которые явно импортируют и экспортируют значения. Разберем, как браузеры (в частности, V8 в Chrome) загружают модули и как в этом процессе используются такие возможности, как динамический импорт (`import()`) и карты импортов (import maps).

__Статические импорт модулей:__ когда браузер встречает тег `<script type="module" src="main.js">`, он рассматривает `main.js` как точку входа модуля. Процесс загрузки выглядит так: браузер загружает `main.js`, затем парсит его как ES-модуль. Во время парсинга он находит все выражения `import` (например, `import { foo } from './utils.js';`). Вместо того, чтобы сразу выполнить код, браузер сначала формирует граф зависимостей модуля. Он начинает загружать все импортированные файлы (в данном случае — `utils.js`) и рекурсивно обрабатывает каждый из них, находя и подгружая их зависимости. Все это происходит асинхронно. Только после того, как весь граф модулей будет загружен и разобран, браузер приступает к его выполнению. Код модулей выполняется отложено — он запускается лишь тогда, когда все зависимости готовы, причем строго в порядке импорта зависимостей (если модуль A импортирует B, то сначала выполняется B).

Именно поэтому ES-модули нельзя просто загружать по протоколу `file://` , если это явно не разрешено, и поэтому для междоменных скриптов требуется поддержка [CORS](https://developer.mozilla.org/ru/docs/Web/HTTP/Guides/CORS). Браузер не просто добавляет `<script>` на страницу — он активно связывает и подгружает несколько файлов, выстраивая между ними зависимости.

__Динамический импорт модулей:__ помимо статических операторов `import`, в стандарте ES2020 появилась функция `import(moduleSpecifier)` — выражение, которое позволяет загружать модули "на лету". Оно возвращает `Promise`, который в итоге предоставляет экспортируемые значения модуля.

Например, можно вызвать: `const module = await import('./analytics.js');` в ответ на действие пользователя — так реализуется разделение кода (code splitting).

Под капотом `import()` заставляет браузер загрузить указанный модуль (и его зависимости, если они еще не загружены), затем выполнить его инициализацию и запуск. После этого промис возвращает объект пространства имен (namespace) модуля. Здесь браузер и V8 работают совместно: браузерный загрузчик модулей отвечает за загрузку и парсинг, а V8 — за компиляцию и выполнение кода.

Преимущество динамического импорта в том, что его можно использовать даже в обычных скриптах, а не только в модульных — например, внутри встроенного `<script>`. Он позволяет загружать код по мере необходимости.

Главное отличие от статического импорта состоит в том, что статические импорты разрешаются заранее — еще до выполнения кода браузер загружает весь граф зависимостей. Динамический же импорт работает скорее как загрузка нового скрипта во время выполнения программы, но при этом сохраняет модульную модель и использует промисы.

__Карты импортов:__ одна из ключевых проблем ES-модулей в браузере долгое время была связана с тем, как именно разрешаются имена модулей. В Node.js или в сборщиках модулей (bundlers) значения можно импортировать по имени пакета (например, `import { useState } from 'react';`). Но в браузере без этапа сборки такой идентификатор не является корректным URL: браузер воспринимает `react` как относительный путь и, разумеется, не может его загрузить.

Здесь на помощь приходят карты импортов. Карта импорта — это конфигурация в формате JSON, которая указывает браузеру, как сопоставлять идентификаторы модулей с реальными URL. Подключается она через HTML тег `<script type="importmap">`. Например, в карте импорта можно указать, что идентификатор `react` должен соответствовать URL: `https://cdn.example.com/react@19.0.0/index.js`. После этого при выполнении `import 'react'` браузер с помощью карты сразу понимает, какой адрес использовать, и загружает нужный модуль. По сути, карты импортов позволяют использовать "голые" (bare) идентификаторы (вроде имен npm-пакетов) прямо в браузере, сопоставляя их с CDN-ресурсами или локальными файлами.

Карты импортов значительно упростили разработку без использования сборщиков. Начиная с 2023 года, они поддерживаются всеми основными браузерами: Chrome 89+, Firefox 108+, Safari 16.4+ — т.е. всеми тремя движками. Особенно полезны они для локальной разработки или небольших приложений, где не хочется подключать сборщик. В продакшене крупные проекты по-прежнему часто собирают в сборки (bundles), чтобы сократить количество запросов, но по мере развития браузеров и с появлением HTTP/2/3 подход с большим числом отдельных модулей становится вполне рабочим и эффективным.

Загрузчик модулей в браузере состоит из карты модулей (отслеживающей, что уже загружено), при необходимости карты импорта (для кастомного разрешения идентификаторов) и логики загрузки и парсинга модулей. После загрузки и компиляции код модуля выполняется в строгом режиме (strict mode) и в собственной области видимости верхнего уровня — ничего не попадает в `window`, если не делать этого специально. Экспортируемые значения кэшируются, поэтому при повторном импорте того же модуля он не выполняется заново, а используется уже готовый результат.

Еще один момент: в отличие от обычных скриптов, ES-модули откладывают выполнение кода до момента готовности зависимостей и выполняются в определенном порядке для каждого графа зависимостей. Например, если `main.js` импортирует `util.js`, а `util.js` импортирует `dep.js`, порядок выполнения будет следующим: сначала `dep.js`, затем `util.js`, и только потом `main.js` ([обход в глубину](https://my-js.org/docs/algorithms-data-structures/algorithms/tree#%D0%BF%D0%BE%D0%B8%D1%81%D0%BA-%D0%B2-%D0%B3%D0%BB%D1%83%D0%B1%D0%B8%D0%BD%D1%83)). Такой детерминированный порядок выполнения позволяет в некоторых случаях обходиться без событий, вроде `DOMContentLoaded`, так как к моменту запуска главного модуля все его зависимости уже загружены и выполнены.

С точки зрения V8, модули обрабатываются той же цепочкой компиляции, что и обычный JS-код, но для каждого модуля создается отдельный объект `ModuleRecord`. Движок гарантирует, что верхнеуровневый код модуля выполняется только после того, как будут готовы все его зависимости.

V8 также умеет работать с циклическими импортами модулей, которые разрешены спецификацией и могут приводить к частично инициализированным экспортам. Подробности определены стандартом, но в целом движок сначала создает все экземпляры модулей, затем решает циклы с помощью временных "заполнителей" (placeholders), а после этого выполняет код модулей в порядке, учитывающем зависимости (алгоритм, описанный в спецификации, представляет собой [топологическую сортировку](https://my-js.org/docs/algorithms-data-structures/algorithms/graph/topological-sort) ориентированного ациклического графа (DAG) модуля).

Загрузка модулей в браузере — это скоординированная работа сети (загрузка файлов модулей), резолвера модулей (с использованием карт импортов или стандартного разрешения URL) и движка JS (компиляция и выполнение модулей в правильном порядке). Этот процесс сложнее, чем простая загрузка через `<script>`, но он обеспечивает более модульную и удобную для поддержки структуру кода. Ключевые выводы для разработчиков: используйте модули для организации кода, применяйте карты импортов для "голых" импортов и помните, что модули можно загружать динамически через `import()`. Браузер берет на себя всю работу по правильному порядку выполнения кода.

Теперь, когда мы разобрались, как работает загрузка и выполнение JS в одной вкладке, можно перейти к устройству браузера как __многопроцессной системы__ — той архитектуре, которая позволяет множеству страниц и приложений работать параллельно, не мешая друг другу.

## Многопроцессная архитектура браузеров

Современные браузеры (Chrome, Firefox, Safari, Edge и др.) используют многопроцессную архитектуру, чтобы повысить стабильность, безопасность и изолировать производительность. Раньше весь браузер работал как единый процесс, но теперь разные его компоненты запускаются отдельно. Chrome стал пионером этого подхода в 2008 году, после чего другие браузеры постепенно внедрили аналогичные решения. Рассмотрим архитектуру Chromium и отметим отличия в Firefox и Safari.

В Chromium (Chrome, Edge, Brave и др.) есть один центральный процесс — __процесс браузера__ (Browser Process). Он отвечает за пользовательский интерфейс (адресная строка, закладки, меню — вся "хромовая" оболочка браузера) и за координацию высокоуровневых задач, таких как загрузка ресурсов и навигация. Когда открывается Chrome, и мы видим один процесс в диспетчере задач ОС, это и есть Browser Process. Он же является родительским процессом, порождающим остальные.

Для каждой вкладки (а иногда и для каждого сайта в отдельной вкладке) Chrome создает __процесс рендеринга__ (Renderer Process). Этот процесс запускает движок рендеринга Blink и движок JS V8 для содержимого конкретной вкладки. В целом, каждой вкладке выделяется, как минимум, один процесс рендеринга.

<img src="https://habrastorage.org/webt/df/_n/wp/df_nwpvppcxu33joi9rjbwpflni.png" />
<br />

Если открыто несколько независимых сайтов, они будут работать в отдельных процессах (например, сайт A в одном, сайт Б в другом и т.д.). Chrome даже помещает `iframe` с другими доменами в отдельные процессы (подробнее об этом в разделе про изоляцию сайтов). Процесс рендеринга работает в песочнице (sandbox) и не может напрямую обращаться к файловой системе или сети — для таких операций ему нужно взаимодействовать с процессом браузера.

Другие важные процессы в Chrome:

* __Процесс GPU:__ процесс, выделенный для работы с GPU, как было описано ранее. Все запросы на рендеринг и компоновку от процессов рендеринга направляются в GPU-процесс, который выполняет реальные вызовы графического API. Этот процесс изолирован в песочнице, чтобы сбой GPU не приводил к падению рендереров.
* __Сетевой процесс:__ в старых версиях Chrome сетевые операции выполнялись в потоке внутри процесса браузера, но теперь это часто отдельный процесс благодаря ["сервисификации" (servicification)](https://www.chromium.org/servicification/). Он обрабатывает сетевые запросы, DNS и т.д., и также может быть изолирован отдельно.
* __Служебные (вспомогательные) процессы:__ используются для разных сервисов, таких как воспроизведение аудио, декодирование изображений и других задач, которые Chrome может выносить в отдельные процессы.
* __Процесс плагинов:__ в эпоху Flash и плагинов NPAPI плагины запускались в собственном процессе. Сейчас Flash устарел, поэтому это менее актуально, но архитектура все еще поддерживает запуск плагинов вне основного процесса браузера.
* __Процессы расширений:__ расширения Chrome (по сути скрипты, которые могут воздействовать на веб-страницы или браузер) тоже запускаются в отдельных процессах, изолированных от сайтов для повышения безопасности.

Если упростить, это выглядит так: один процесс браузера управляет несколькими процессами рендеринга (по одному на вкладку или на отдельный сайт), а также процессом GPU и несколькими вспомогательными сервисными процессами. В диспетчере задач Chrome (Shift+Esc на Windows или через "Дополнительные инструменты" → "Диспетчер задач") можно увидеть каждый процесс с указанием его типа и объема потребляемой памяти.

__Преимущества многопроцессной архитектуры:__

* __Стабильность:__ если вкладка с веб-страницей упадет или начнет расходовать слишком много памяти, это не обрушит весь браузер — достаточно закрыть только эту страницу, и остальные вкладки продолжат нормально работать. В однопроцессных браузерах один проблемный скрипт мог положить весь браузер. В Chrome при сбое процесса вкладки появляется ошибка "Aw, Snap", и ее можно перезагрузить отдельно.
* __Безопасность (песочница):__ запуская веб-контент в изолированном процессе, браузер может ограничить его возможности в вашей системе. Даже если злоумышленник обнаружит уязвимость в движке рендеринга, он останется в песочнице — процесс рендеринга обычно не может читать файлы, открывать сетевые соединения или запускать программы напрямую. Для доступа к таким ресурсам требуется запрос к процессу браузера, который может его подтвердить или отклонить. Песочница реализована на уровне ОС (через объекты заданий (job objects), seccomp-фильтры и т.д. в зависимости от платформы).
* __Изоляция производительности:__ тяжелая работа в одной вкладке (сложное веб-приложение или бесконечный цикл) ограничена процессом этой вкладки. Другие вкладки продолжают работать нормально, потому что их процессы не блокируются. ОС может распределять их по разным ядрам процессора, что позволяет эффективно выполнять несколько "тяжелых" страниц одновременно — куда лучше, чем если бы все шло в потоках одного процесса.
* __Сегментация памяти:__ каждый процесс имеет собственное адресное пространство, память не разделяется. Это предотвращает доступ одного сайта к данным другого и позволяет ОС полностью освобождать память при закрытии вкладки. Минус — небольшой "оверхед" из-за дублирования ресурсов (каждый рендеринг загружает свою копию движка JS и других необходимых компонентов).
__Изоляция сайтов:__ изначально в Chrome использовалась модель "один процесс на вкладку". Со временем ее сменили на "один процесс на сайт" (особенно после уязвимости Spectre — подробнее в следующем разделе про безопасность). По состоянию на 2024 год изоляция сайтов включена по умолчанию у 99% пользователей Chrome на десктопах, а поддержка на Android все еще активно дорабатывается. Это означает, что если у нас открыто две вкладки с example.com, Chrome может использовать один процесс для обеих, чтобы экономить память — ведь это один и тот же сайт, и объединять их достаточно безопасно. Но если на странице example.com есть `iframe` с evil.com, Chrome обязатель­но вынесет `iframe` в отдельный процесс, чтобы защитить данные основного сайта. Такой подход называется строгой изоляцией сайтов (Strict Site Isolation) и стал включаться по умолчанию, начиная примерно с Chrome 67. Изоляция сайтов увеличивает использование системных ресурсов примерно на 10–13% из-за большего числа создаваемых процессов, однако обеспечивает критически важный уровень безопасности.

Архитектура Firefox, называемая [Electrolysis](https://blog.mozilla.org/addons/2016/04/11/the-why-of-electrolysis/) (e10s), долгое время использовала всего один контент-процесс для всех вкладок (много лет Firefox был однопроцессным браузером и лишь около 2017 года начал включать несколько процессов). Начиная с 2021 года Firefox использует несколько процессов для веб-контента (по умолчанию — 8). С запуском [Project Fission](https://blog.mozilla.org/security/2021/05/18/introducing-site-isolation-in-firefox/) (изоляция сайтов) Firefox движется к той же модели, что и Chrome: он может выделять отдельные процессы для межсайтовых `iframe`, и начиная с версии Firefox 108+ изоляция сайтов включена по умолчанию. Это позволяет браузеру при необходимости запускать практически один процесс на сайт, подобно Chrome. Firefox также имеет GPU-процесс (используется WebRender и компоновка) и отдельный сетевой процесс, что похоже на разделение в Chrome. В итоге, архитектура Firefox сейчас очень близка к модели Chrome: родительский процесс, GPU-процесс, сетевой процесс, несколько процессов контента (рендереров) и ряд служебных процессов (например, для расширений, декодирования медиа — медиаплагин может работать изолированно).

Safari (WebKit) также перешел на многопроцессную архитектуру (WebKit2), где любое содержимое вкладки запускается в отдельном процессе WebContent, а центральный UI-процесс управляет ими. WebContent-процессы Safari также изолированы и не могут напрямую обращаться к устройствам или файлам без участия UI-процесса. У Safari есть и сетевой процесс (а также несколько других вспомогательных). Таким образом, несмотря на различия в реализации, концепция везде одинакова: каждая веб-страница выполняется в своем изолированном процессе-песочнице.

Важный момент — межпроцессное взаимодействие (IPC): как процессы обмениваются данными? Браузеры используют механизмы IPC (в Windows это часто именованные каналы (named pipes) или другие средства ОС; в Linux — Unix-сокеты или общая память; у Chrome есть собственная IPC-библиотека Mojo). Например, когда сетевой ответ приходит в сетевой процесс, его нужно передать нужному процессу рендеринга (при участии браузерного процесса, который координирует передачу). Аналогично, когда в JS вызывается `fetch()`, движок обращается к сетевому API, который отправляет запрос в сетевой процесс и т.д. IPC добавляет сложности, но браузеры активно оптимизируют взаимодействие (например, используют общую память для эффективной передачи больших данных, таких как изображения, и отправляют асинхронные сообщения, чтобы избежать блокировок).

__Стратегии распределения процессов:__ Chrome не всегда создает новый процесс для каждой вкладки — существуют ограничения, особенно на устройствах с малым объемом памяти. В таких случаях браузер может повторно использовать процесс для вкладок одного и того же сайта. Например, если открыть еще одну вкладку того же сайта, Chrome может использовать уже существующий рендерер, чтобы сэкономить память. Поэтому иногда две вкладки одного и того же сайта работают в одном процессе. Кроме того, существует общий лимит на количество процессов, который может масштабироваться в зависимости от объема оперативной памяти (RAM). При достижении лимита, браузер может объединять несколько несвязанных сайтов в одном процессе, хотя при включенной изоляции сайтов старается этого избегать. На Android Chrome использует меньше процессов из-за ограничений памяти — обычно максимум 5–6 процессов для контента.

Еще одна концепция в Chromium — __сервисификация__: разделение компонентов браузера на сервисы, которые могут работать в отдельных процессах. Например, сетевой сервис (Network Service) вынесен в отдельный модуль, который может выполняться вне основного процесса. Идея в модульности: на мощных устройствах каждый сервис может работать в своем процессе, тогда как на устройствах с ограниченными ресурсами некоторые сервисы могут объединяться в один процесс, чтобы уменьшить накладные расходы. Chrome может решать, как развернуть эти сервисы — во время работы или на этапе сборки. На мощных устройствах процессы разделяются полностью (UI, сеть, GPU и т.д. — все отдельно), а на слабых (Android) браузер и сетевой сервис могут работать в одном процессе, чтобы сократить нагрузку.

Вывод: архитектура Chromium рассчитана на то, чтобы интерфейс браузера и каждый сайт работали в отдельных песочницах, используя процессы как границу изоляции. Firefox и Safari со временем пришли к похожим решениям. Такая архитектура значительно повышает безопасность и надежность, хотя требует больше памяти. Процессы веб-контента считаются небезопасными, и именно здесь вступает в силу изоляция сайтов, позволяющая изолировать даже разные домены друг от друга в отдельных процессах.

## Песочницы и изоляция сайтов

Песочницы и изоляция сайтов — это функции безопасности, которые строятся на многопроцессной архитектуре. Их цель — сделать так, чтобы даже если в браузере выполнится вредоносный код, он не смог легко получить данные с других сайтов или получить доступ к системе.

__Изоляция сайтов:__ мы уже затрагивали этот момент — она означает, что разные сайты запускаются в отдельных процессах рендеринга. После обнаружения [уязвимости Spectre](https://developer.chrome.com/blog/meltdown-spectre) в 2018 году Chrome усилил эту защиту. Spectre показал, что вредоносный JS потенциально мог читать чужую память через спекулятивное выполнение CPU. Если два сайта находились в одном процессе, вредоносный сайт мог использовать Spectre, чтобы подглядывать в память конфиденциального сайта (например, банковского). Единственное надежное решение — не позволять сайтам делить процесс вообще. Поэтому Chrome сделал изоляцию сайтов включенной по умолчанию: каждый сайт получает свой процесс, включая `iframe` с другими доменами. Firefox пошел тем же путем с Project Fission (по умолчанию включен в последних версиях), стремясь к той же цели — каждый сайт в отдельном процессе ради безопасности. Это существенное изменение по сравнению с прошлым, когда родительская страница и несколько `iframe` с разными доменами могли жить в одном процессе (особенно в одной вкладке). Теперь `iframe`, вроде `<iframe src="https://evil.com">`, на странице "хорошего" сайта принудительно помещается в отдельный процесс, что предотвращает утечку данных даже при низкоуровневых атаках.

С точки зрения разработчика, изоляция сайтов в основном прозрачна. Одно из последствий — коммуникация между встроенным `iframe` и родительской страницей может пересекать границы процессов, поэтому такие методы как `postMessage()` реализуются через IPC. Но браузер делает это незаметно — для разработчика API остаются привычными и работают, как обычно.

__Песочница:__ каждый процесс рендеринга (и другие вспомогательные процессы) работает в песочнице с ограниченными возможностями. Например, в Windows Chrome использует объект задач и снижает права, чтобы рендер не мог вызывать большинство Win32 API, имеющих доступ к системе. В Linux применяются пространства имен и фильтры Seccomp для ограничения системных вызовов. Процесс рендеринга может выполнять вычисления и отображать контент, но если он попытается открыть файл, камеру или микрофон — доступ будет заблокирован (если только не использовать легальные каналы через процесс браузера с запросом разрешения у пользователя).

Документация WebKit явно указывает, что процессы WebContent не имеют прямого доступа к файловой системе, буферу обмена, устройствам и т.д. — они должны делать запрос через процесс интерфейса браузера, который выполняет посредническую функцию. Именно поэтому, когда сайт запрашивает доступ к микрофону, окно разрешения отображается UI браузера (процесс браузера), а сама запись звука выполняется в контролируемом процессе.

Песочница — критически важный барьер защиты. Даже если злоумышленник найдет уязвимость для выполнения нативного кода в рендерере, он столкнется с ограничениями песочницы — потребуется отдельный эксплойт (так называемый "escape"), чтобы выйти в систему. Этот многоуровневый подход (изоляция сайтов + песочница) является передовым стандартом безопасности в браузерах.

Песочница Firefox также довольно серьезная. До e10s защита была слабее, но со временем ее усилили. Процессы контента Firefox тоже не имеют прямого доступа к системе, а процесс GPU дополнительно изолирован для безопасной работы с драйверами графики.

__Процессно-изолированные фреймы (Out-of-Process iframes, OOPIF):__ в реализации изоляции сайтов в Chrome был введен термин [OOPIF](https://www.chromium.org/developers/design-documents/oop-iframes/) — `iframe` вне процесса. С точки зрения пользователя ничего не меняется, но в архитектуре Chrome каждый фрейм страницы может работать в отдельном процессе рендеринга. Основной фрейм и фреймы того же сайта делят один процесс, а межсайтовые фреймы используют отдельные процессы. Все эти процессы "сотрудничают", чтобы отобразить содержимое одной вкладки, при этом их координирует процесс браузера. Это довольно сложная архитектура, но Chrome использует дерево фреймов, которое может распространяться на несколько процессов.

На практике это означает, что одна вкладка может запускать несколько процессов: один для основного документа и отдельные — для каждого межсайтового поддокумента. Процессы общаются через IPC, например, для передачи событий DOM через границы процессов или для определенных JS-вызовов между контекстами. Веб платформа (через спецификации вроде [COOP/COEP](https://web.dev/articles/coop-coep), [SharedArrayBuffer](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/SharedArrayBuffer) и др.) развивается с учетом этих ограничений после уязвимости Spectre.

__Память и производительность:__ изоляция сайтов действительно увеличивает расход памяти, так как задействуется больше процессов. Разработчики Chrome отмечают, что в некоторых случаях это может давать 10–20% [дополнительной нагрузки](https://www.thurrott.com/mobile/chrome-os/162980/spectre-mitigation-increases-chrome-memory-usage-google-says) на память. Часть этой нагрузки они компенсировали с помощью так называемой "консолидации процессов по принципу наилучших усилий" для вкладок одного сайта и ограничения количества создаваемых процессов (было упомянуто ранее). В Firefox изначально не изолировали каждый сайт из-за ограничений памяти, но после уязвимости Spectre нашли способы делать это эффективнее — с ограничением до 8 привилегированных процессов и динамическим созданием процессов по мере необходимости. Safari исторически использовал сильную многопроцессную модель, хотя не совсем ясно, изолирует ли он межсайтовые `iframe`; WebKit2 точно изолирует верхнеуровневые страницы. При этом Apple уделяет особое внимание конфиденциальности (например, "Технология интеллектуальной защиты от отслеживания" (Intelligent Tracking Prevention) разделяет куки), но это уже отдельный уровень.

Предзагрузка страниц с другого сайта (cross-site prefetch) ограничена из соображений конфиденциальности и работает, только если у пользователя нет куки для целевого сайта. Это предотвращает слежку за действиями пользователя через предзагруженные страницы, которые он может и не посетить.

В целом, изоляция сайтов обеспечивает соблюдение принципа минимальных привилегий: код с домена A не может получить доступ к данным с домена B, только через веб-API с явного согласия пользователя (например, через `postMessage()` или общее хранилище). Песочница гарантирует, что даже если код окажется вредоносным, он не сможет напрямую повлиять на систему. Эти меры значительно усложняют эксплуатацию уязвимостей: злоумышленнику теперь нужно использовать цепочку нескольких эксплойтов (например, один для взлома рендерера, другой для выхода из песочницы), чтобы нанести серьезный ущерб.

Для веб-разработчика изоляция сайтов почти незаметна, но она делает работу с вебом безопаснее. Единственное, что стоит учитывать: межпроцессное взаимодействие между сайтами может давать небольшую дополнительную нагрузку (из-за IPC), а некоторые оптимизации, вроде совместного использования скриптов в одном процессе, между разными доменами невозможны. Браузеры постоянно оптимизируют интерфейс обмена сообщениями между процессами, чтобы минимизировать потери производительности.

## Сравнение Chromium, Gecko и WebKit

Мы в основном описывали поведение Chrome/Chromium (движок Blink для HTML/CSS, V8 для JS, многопроцессная архитектура через инфраструктуру Aura/Chromium). Другие крупные движки — Gecko от Mozilla (используется в Firefox) и WebKit от Apple (используется в Safari) — преследуют те же базовые цели и имеют схожий конвейер обработки, но есть важные отличия и исторические расхождения.

__Общие концепции:__ все движки разбирают HTML в DOM, CSS — в данные о стилях, вычисляют расположение элементов и выполняют рендеринг/компоновку. Все имеют движки JS с JIT-компиляцией и сборкой мусора. И все современные движки используют многопроцессность (или, как минимум, многопоточность) для параллельной работы и повышения безопасности.

### Отличия в CSS/системе стилей

Интересное различие заключается в том, как движки реализуют вычисление стилей CSS:

* __Blink (Chromium):__ использует однопоточный движок стилей на C++ (основанный на WebKit). Стили вычисляются последовательно для дерева DOM. Существуют оптимизации с инкрементальной проверкой валидности стилей, но в целом работу выполняет один поток (за исключением небольшой параллелизации для анимации).
* __Gecko (Firefox):__ в рамках проекта Quantum (2017) Firefox интегрировал Stylo — новый CSS-движок на Rust с поддержкой многопоточности. Firefox может вычислять стили для разных поддеревьев DOM параллельно, используя все ядра процессора. Это стало значительным улучшением производительности CSS в Gecko: перерасчет стилей может использовать 4 ядра для работы, которую Blink выполняет на одном. Главный плюс подхода Gecko — скорость, но цена — сложность реализации.
* __WebKit (Safari):__ движок стилей WebKit, как и Blink, однопоточный (Blink отделился от WebKit в 2013 году, до этого архитектура была общей). В WebKit реализованы интересные решения, например, JIT-компиляция байт-кода для CSS-селекторов: селекторы могут трансформироваться в байт-код, а затем компилироваться JIT для ускорения сопоставления. Blink такой подход не использует, применяя итеративное сопоставление.

Таким образом, в CSS движок Gecko выделяется параллельным вычислением стилей с использованием Rust, тогда как Blink и WebKit полагаются на оптимизированный C++ и некоторые JIT-трюки (в случае WebKit).

### Разметка и графика

Все три движка реализуют модель коробки (box model) CSS и алгоритмы разметки. Некоторые возможности могут появляться в одном движке раньше, чем в других (например, когда-то WebKit опережал в поддержке CSS Grid, затем Blink его догнал — часто они синхронизируются через стандартизацию).

Firefox (Gecko) сделал огромный шаг, внедрив __WebRender__ как движок разметки и растеризации. Сейчас WebRender — рендерер по умолчанию в Firefox. Он заметно улучшил производительность, особенно при работе с графически насыщенным веб-контентом. WebRender (также написанный на Rust) принимает список отображаемых элементов и непосредственно рендерит его на GPU, обрабатывая с помощью графического процессора такие задачи, как тесселяция фигур, текста и т.д. По сути, он переносит большую часть работы по отрисовке страницы на GPU.

В Chrome большая часть растеризации по-прежнему выполняется на CPU, а затем отправляется на GPU в виде битовых карт (bitmaps). WebRender старается избегать создания битмапов для целых слоев и вместо этого рисует векторное содержимое прямо на GPU (за исключением глифов текста, которые он кэширует в атласных текстурах (atlas textures)). Это позволяет Firefox потенциально анимировать больше элементов с высокой производительностью — ему не нужно заново растрировать все содержимое при изменении лишь небольших его участков; перерисовка с помощью GPU выполняется очень быстро. Это похоже на то, как игровой движок обновляет кадр: каждый фрейм строится через GPU-вызовы. Минус: подход сложен в реализации и требует тонкой настройки, а также сильнее нагружает видеокарту. Однако с ростом вычислительной мощности GPU этот подход становится все более перспективным. Команда Chrome рассматривала похожую идею (SKIA GPU), но не провела полного пересмотра архитектуры в стиле WebRender.

Safari (WebKit) использует подход, более похожий на тот, что был у старых версий Chrome: он формирует разметку через слои (называемые CALayer, поскольку на macOS и iOS используется система слоев Core Animation). Safari одним из первых перешел к GPU-разметке — еще в iPhone OS и Safari 4 (в 2009 году) некоторые CSS-эффекты, такие как трансформации, аппаратно ускорялись на GPU. В дальнейшем Safari и Chrome разошлись в реализации, но концептуально оба используют плиточную структуру (tiling) и разметку. Safari также активно выносит задачи на GPU и использует тайлинг — особенно на iOS, где отрисовка с помощью плиток изначально стала основой для обеспечения плавной прокрутки.

__Оптимизации для мобильных устройств:__ каждый движок имеет свои особенности для мобильных платформ. WebKit использует концепцию покрытия плитками для прокрутки (исторически применялось в iOS через UIWebView). Chrome на Android использует тайлинг и старается минимизировать задачи растеризации, чтобы выдерживать частоту кадров. Firefox применяет WebRender, который изначально разрабатывался в рамках проекта Servo.

### JavaScript-движки

* __V8 (Chromium):__ мы уже описывали его уровни: Ignition, Sparkplug, TurboFan и Maglev (по состоянию на 2023 год).
* __SpiderMonkey (Firefox):__ исторически имел интерпретатор, затем Baseline JIT и оптимизирующий компилятор IonMonkey. Недавний проект Warp изменил устройство уровней JIT — он упростил работу Ion и сделал ее больше похожей на подход V8 TurboFan по использованию кэшированного байт-кода и информации о типах. SpiderMonkey также использует другой сборщик мусора (тоже поколенческий — с 2012 года он называется Incremental GC, сейчас в основном инкрементальный и частично конкурентный).
* __JavaScriptCore (Safari):__ как уже было отмечено, включает четыре уровня: LLInt, Baseline, DFG и FTL. Использует другой сборщик мусора (в WebKit ранее применялся поколенческий сборщик на основе маркировки и очистки (mark-sweep) — так называемый Butterfly или разновидности Boehm; в настоящее время используется bmalloc и др.). Уровень FTL опирается на LLVM — это уникально (V8 и SpiderMonkey используют собственные компиляторы). Такой подход может обеспечивать очень высокую скорость, но требует тяжелой компиляции. JSC иногда показывает выдающиеся результаты на отдельных бенчмарках, хотя V8 часто быстро догоняет — движки регулярно обгоняют друг друга.

По поддержке возможностей ECMAScript все три движка в целом идут в ногу со стандартами — благодаря [test262](https://v8.github.io/test262/website/default.html) и сильной конкуренции между собой.

### Отличия в многопроцессной архитектуре

* __Chrome:__ обычно использует отдельный процесс для каждой вкладки; при изоляции сайтов процессы разделяются на уровне источника (домена), поэтому их может быть очень много (иногда десятки).
* __Firefox:__ по умолчанию использует меньше процессов — примерно, 8 контент-процессов на все вкладки, плюс дополнительные, при необходимости (например, для межсайтового `iframe` при включенном Fission). Это означает, что Firefox не выделяет отдельный процесс для каждой вкладки: вкладки делят процессы из общего пула. Такой подход снижает потребление памяти при большом числе вкладок, но в то же время это означает, что сбой одного процесса может сломать сразу несколько вкладок (хотя Firefox пытается группировать их по сайтам, например, все вкладки Facebook — в одном процессе).
* __Safari:__ обычно создает один процесс на вкладку (или на несколько вкладок). На iOS каждый WebView точно изолируется отдельно. На десктопном Safari ранее также использовалась отдельная изоляция для каждой вкладки. Не до конца ясно, изолируются ли `iframe` с разными доменами — Apple не слишком подробно описывает меры против Spectre, но, по крайней мере, для доменов верхнего уровня изоляция есть.

__Межпроцессное взаимодействие:__ всем движкам приходится решать похожие задачи — например, как реализовать `alert()` (блокирующий выполнение JS) в многопроцессной среде. Обычно основной процесс браузера отображает диалоговое окно оповещения и приостанавливает соответствующий контекст выполнения скрипта. Аналогично обстоит дело с обработкой `prompt()`, `confirm()` и реализацией модальных окон. Однако между браузерами есть нюансы реализации: например, в Chrome блокировка потока при вызове `alert()` не является "настоящей" — рендерер запускает вложенный цикл обработки событий, тогда как Firefox может полностью приостанавливать процесс соответствующей вкладки.

__Обработка сбоев:__ Chrome и Firefox имеют собственные системы отчетов о сбоях, способные перезапустить аварийно завершившийся процесс контента и отобразить сообщение об ошибке во вкладке. При сбое процесса Web Content в Safari, как правило, отображается более простое сообщение об ошибке непосредственно в области содержимого.

### Расхождения в реализации возможностей

Некоторые возможности веб-платформы зависят от конкретного движка: например, Chrome поддерживает экспериментальный API `document.transition()` для плавных переходов DOM, который опирается на архитектуру Blink. Firefox, возможно, реализует аналогичную функциональность с другим подходом. Однако со временем стандарты приводят такие функции к общему виду (_прим. пер.:_ кажется, это уже произошло через [View Transition API](https://developer.mozilla.org/en-US/docs/Web/API/View_Transition_API)).

__Инструменты разработчика:__ DevTools в Chrome отличаются высокой степенью проработанности. В Firefox инструменты разработчика тоже на высоком уровне и имеют уникальные возможности (например, инструмент подсветки CSS Grid, редактор форм). Web Inspector в Safari вполне работоспособен, но в отдельных аспектах уступает по функциональности. Эти различия важны для разработчиков при отладке в разных браузерах.

### Компромиссы производительности

Исторически Chrome считался самым быстрым благодаря своей многопроцессной архитектуре и движку V8. После выхода Firefox Quantum разрыв значительно сократился, и в некоторых задачах Firefox даже обгоняет Chrome, особенно в графике (WebRender показывает отличную производительность на сложных страницах). Safari зачастую демонстрирует высокие показатели в графике и энергоэффективности на устройствах Apple (здесь производитель уделяет большое внимание оптимизации энергопотребления).

__Память:__ Chrome известен высоким расходом памяти из-за большого количества процессов. Firefox обычно более экономичен. Safari на iOS крайне бережно относится к памяти — это необходимость при ограниченном объеме RAM, поэтому в WebKit реализовано множество механизмов оптимизации памяти.

__Внешние разработчики:__ забавно, но многие улучшения в браузерных движках создаются сторонними командами, такими как Igalia (например, реализация CSS Grid в Blink и WebKit). Благодаря этому некоторые возможности появляются в разных браузерах почти одновременно.

С точки зрения веб-разработчика различия между движками проявляются в следующем:

* необходимость тестировать на всех движках, поскольку возможны незначительные различия или ошибки в реализации того или иного свойства CSS или API
* различия в производительности: например, определенный код JS может выполняться быстрее в одном движке по сравнению с другим из-за особенностей работы JIT
* отсутствие поддержки некоторых API в отдельных браузерах (например, Safari зачастую последним внедряет новые возможности, такие как WebRTC или современные версии IndexedDB)

Но основные концепции, которые мы рассмотрели (сеть → парсинг → макет → отрисовка → композиция → выполнение JS), применимы ко всем движкам — различаются лишь внутренние реализации и названия этапов:

* В Gecko: парсинг → дерево фреймов → список отображения → сцена WebRender или дерево слоев (если WebRender отключен) → композиция
* В WebKit: парсинг → дерево рендеринга → графические слои → композиция (через Core Animation)

И у всех есть аналогичные подсистемы: DOM, стилизация, макет, графика, движок JS, сетевые модули, процессы/потоки.

Понимание этих этапов помогает в отладке: например, если анимация "лагает" в Safari, но не в Chrome, это может быть связано с отличиями в механизме отрисовки WebKit. А если стили CSS обрабатываются медленно в Firefox, возможно, задействован участок, который не параллелизуется движком Stylo (хотя такие случаи встречаются редко).

Подводя итог, можно сказать: хотя Chromium, Gecko и WebKit имеют разные реализации и собственные инновации (например, параллельное вычисление стилей в Gecko, WebRender на GPU и т.д.), они все активнее движутся к единым стандартам и даже совместным разработкам. Выбор движка больше важен для производителей платформ и разнообразия в экосистеме, а для веб-разработчика, в первую очередь, важно, чтобы сайт корректно работал везде. Уникальная архитектура каждого движка может приводить к различиям в производительности или проявлению специфических ошибок. Именно поэтому тестирование в разных браузерах и использование их встроенных инструментов диагностики дает ценное понимание поведения приложения. Перечислить все отличия в рамках одной статьи невозможно, но, надеюсь, что этот обзор помог сформировать общее представление: на высоком уровне движки похожи (многопроцессность, общая конвейерная модель), однако сохраняют различия в конкретных технических решениях.

## Заключение и дополнительные материалы

Мы прошли весь путь жизни веб-страницы внутри современного браузера — от ввода URL до сетевого взаимодействия и навигации, парсинга HTML, применения стилей, формирования макета, отрисовки и выполнения JS, вплоть до вывода пикселей на экран с помощью GPU. Мы убедились, что браузеры по сути представляют собой миниатюрные ОС: они управляют процессами, потоками, памятью и множеством сложных подсистем, обеспечивая быструю загрузку и безопасное выполнение кода. Понимание этих внутренних механизмов помогает веб-разработчику осознать, почему те или иные рекомендации действительно важны: например, минимизация перерисовок или использование асинхронных скриптов — критичны для производительности; а политики безопасности, такие как запрет смешивания источников во фреймах, существуют не случайно, а ради защиты пользователя.

Несколько ключевых выводов для разработчиков:

* __Оптимизируйте использование сети:__ меньше запросов и меньший размер файлов = быстрое начало рендеринга. Браузер многое делает сам (HTTP/2, кэширование, спекулятивная загрузка), но можно помочь ему с помощью подсказок для ресурсов и грамотной политики кэширования. Сетевой стек является высокопроизводительным, но задержка является ключевой.
* __Структурируйте HTML и CSS эффективно:__ хорошо организованный DOM и лаконичная CSS-структура (без чрезмерно глубоких деревьев и излишне сложных селекторов) упрощают парсинг и вычисление стилей. Помните, что CSS и DOM формируют вычисленные стили, после чего вычисляется геометрия — значительные изменения DOM или стилей могут вызывать пересчеты.
* __Группируйте обновления DOM,__ чтобы избежать многократных пересчетов стилей и разметки. Используйте вкладку "Производительность" в DevTools, чтобы выявить случаи, когда ваш скрипт вызывает лишние пересчеты компоновки или отрисовки.
* __Для анимаций используйте свойства CSS, совместимые с композицией:__ анимация свойств `transform` или `opacity` остается вне основного потока и обрабатывается композитором (compositor), что обеспечивает плавность. По-возможности, избегайте анимирования свойств, влияющих на компоновку.
* __Следите за выполнением JS:__ несмотря на высокую скорость работы движков JS, длительные задачи блокируют основной поток. Разбивайте тяжелые операции на части, чтобы интерфейс оставался отзывчивым. В некоторых случаях стоит использовать веб-воркеры (Web Workers) для выполнения фоновых задач. Также помните, что интенсивное выполнение JS может вызывать паузы при сборке мусора (в наше время они редки и недолговременны, но могут возникать при значительном увеличении объема используемой памяти).
* __Используйте механизмы безопасности__ осознанно — например, применяйте `sandbox` для `<iframe>` или `rel=noopener` для `<a>`, когда это уместно. Теперь вы знаете, что браузер и так изолирует эти элементы — поддержка его механизмов улучшает безопасность и производительность.
* __DevTools — ваш лучший помощник:__ особенно вкладки Performance и Network, которые четко показывают, что именно делает браузер. Если что-то работает медленно или рывками, инструменты разработчика часто сразу указывают на причину — долгую компоновку, медленную отрисовку и т.д.
* __Для тех, кто хочет погрузиться еще глубже, отличным ресурсом будет книга "Browser Engineering" Павла Панчеки и Криса Харрелсона (доступна на [browser.engineering](https://browser.engineering/)).__ Это бесплатная онлайн-книга, где вы шаг за шагом создаете простой веб-браузер: с понятным изложением сетевого взаимодействия, парсинга HTML/CSS, компоновки и других тем. Она может стать отличным дополнением к этой статье и поможет закрепить знания на практике.

Кроме того, серия статей от команды Chrome ["Inside look at modern web browser"](https://developer.chrome.com/blog/inside-browser-part1) предлагает доступный обзор с наглядными схемами. Блог V8 ([v8.dev](https://v8.dev/)) и [блог Mozilla Hacks](https://hacks.mozilla.org/) — ценные источники о новейших технологиях движков (новые уровни JIT-компиляции или внутреннее устройство WebRender).

В заключение: современные браузеры — это настоящие шедевры инженерной мысли. Они умело скрывают сложность за простым интерфейсом — мы пишем HTML, CSS и JS, доверяя браузеру все остальное. Но заглянув под капот, мы начинаем понимать, почему одни приемы улучшают производительность, а другие — нет: почему важно не блокировать основной поток, почему стоит избегать избыточной сложности DOM. Мы перестаем просто следовать рекомендациям — и начинаем осознанно выбирать решения. В следующий раз, когда вы будете отлаживать страницу или зададитесь вопросом, почему Chrome или Firefox ведут себя так, а не иначе — у вас уже будет четкая внутренняя модель того, как работает браузер.

Удачи в разработке! Помните: глубина веб-платформы щедро вознаграждает тех, кто стремится ее понять — всегда есть, что узнавать, и инструменты, которые помогут вам в этом.

### Дополнительные материалы

* [__Web Browser Engineering__](https://browser.engineering/) — книга о том, как браузеры работают изнутри
* [__Chromium University__](https://www.youtube.com/playlist?list=PL9ioqAuyl6ULp1f36EEjIN1vSBEfsb-0a) — бесплатная серия видео с глубокими разборами внутренних механизмов Chromium (особенно классный доклад - [Life of a Pixel](https://www.youtube.com/watch?v=K2QHdgAKP-s&list=PL9ioqAuyl6ULp1f36EEjIN1vSBEfsb-0a&index=4))
* [__Inside the Browser (серия в блоге Chrome Developers)__](https://developer.chrome.com/blog/inside-browser-part1) — четыре части об архитектуре, навигации, рендеринге и потоках ввода
* [__Google Chrome at 17__](https://addyosmani.com/blog/chrome-17th/) — история развития браузера Chrome
