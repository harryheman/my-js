---
sidebar_position: 24
title: Планировщик Go
description: Планировщик Go
keywords: [go, golang, scheduler, internals]
tags: [go, golang, scheduler, internals]
---

# Планировщик Go

- [Источник](https://nghiant3223.github.io/2025/04/15/go-scheduler.html)

> Эта статья посвящена языку программирования [Go 1.24](https://tip.golang.org/doc/go1.24), работающему на [Linux](https://en.wikipedia.org/wiki/Linux) на архитектуре [ARM](https://en.wikipedia.org/wiki/ARM_architecture_family). Она может не охватывать специфические для других операционных систем (ОС) или аппаратных архитектур детали.

В этой статье подробно рассматриваются следующие вопросы:

- Компиляция и среда выполнения Go
- Примитивный планировщик
- Улучшение планировщика
- Модель GMP
- Начальная загрузка программы
- Создание горутины
- Цикл планирования
- Поиск готовой к выполнения горутины
- Вытеснение горутин
- Обработка системных вызовов
- Сетевой и файловый ввод-вывод
- Работа `netpoll`
- Сборщик мусора
- Общие функции
- API среды выполнения Go

## Введение

> Предполагается, что вы обладаете базовым пониманием параллельного программирования в Go (горутины (goroutines), каналы (channels) и т.д.).

Go или Golang, представленный в 2009 году, неуклонно растет в популярности как язык программирования для разработки многопоточных (concurrent) приложений. Он спроектирован, чтобы быть простым, эффективным и легким в использовании.

Модель параллелизма Go основана на концепции горутин - простых пользовательских потоках (threads), управляемых средой выполнения (runtime) Go в пользовательском пространстве. Go предоставляет полезные примитивы для синхронизации, такие как каналы, чтобы помочь разработчикам эффективно писать код для параллельных вычислений.

Понимание того, как работает планировщик задач (scheduler) Go, является критически важным для разработчиков, которые хотят писать эффективные многопоточные программы. Это также помогает в устранении проблем с производительностью программ и ее оптимизации. В этой статье рассматривается, как планировщик Go развивался со временем и как работает код Go, который мы пишем.

## Компиляция и среда выполнения Go

Компиляция программы на Go происходит в три этапа:

1. __Компиляция__ (compilation): исходные файлы (`*.go`) компилируются в файлы ассемблера (`*.s`).
2. __Сборка__ (assembling): файлы ассемблера собираются в объектные файлы (`*.o`).
3. __Связывание__ (linking): объектные файлы (`*.o`) связываются вместе для создания единого исполняемого двоичного файла.

<img src="https://habrastorage.org/webt/tz/z2/99/tzz299zm6qxpv6kufmacntwpo3u.png" />
<br />

Понимание планировщика Go невозможно без понимания среды выполнения Go. Среда выполнения - это ядро языка программирования, предоставляющее основные функции, такие как планирование, управление памятью и структуры данных. Проще говоря, это набор функций и структур данных, обеспечивающих работу программы. Реализация среды выполнения находится в пакете [runtime](https://github.com/golang/go/tree/go1.24.0/src/runtime). Она написана на Go и ассемблерном коде, причем, последний используется в основном для выполнения низкоуровневых операций, таких как работа с регистрами (registers).

<img src="https://habrastorage.org/webt/03/5t/vd/035tvdybge5td-bi9a91-60pd-o.png" />
<br />

При компиляции некоторые ключевые слова и встроенные функции заменяются компилятором на вызовы функций среды выполнения. Например, ключевое слово `go`, используемое для запуска новой горутины, заменяется вызовом функции [runtime.newproc](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L5014-L5030), а функция `new`, используемая для выделения памяти для нового объекта, заменяется вызовом функции [runtime.newobject](https://github.com/golang/go/blob/go1.24.0/src/runtime/malloc.go#L1710-L1715).

Возможно, вас удивит тот факт, что некоторые функции среды выполнения не имеют реализации на Go. Например, функция [getg](https://github.com/golang/go/blob/go1.24.0/src/runtime/stubs.go#L28-L31) заменяется низкоуровневым ассемблерным кодом во время компиляции. Другие функции, такие как [gogo](https://github.com/golang/go/blob/go1.24.0/src/runtime/stubs.go#L214-L214), зависят от платформы и полностью реализованы на ассемблере. Задача компоновщика (linker) Go - связать эти ассемблерные реализации с их объявлениями на Go.

В некоторых случаях функция не имеет реализации в своем пакете, но связана с определением в среде выполнения с помощью директивы компилятора [//go:linkname](https://pkg.go.dev/cmd/compile#hdr-Linkname_Directive). Например, часто используемая функция [time.Sleep](https://github.com/golang/go/blob/go1.24.0/src/time/sleep.go#L12-L14) связана таким образом со своей фактической реализацией в [runtime.timeSleep](https://github.com/golang/go/blob/go1.24.0/src/runtime/time.go#L297-L340).

## Примитивный планировщик

> Планировщик - это не отдельный объект, а набор функций, облегчающих планирование. Он работает не в отдельном потоке, а в тех же потоках, что и горутины.

Возможно, вам уже знакомы модели многопоточности. Они определяют, как потоки пользовательского пространства (корутины в Kotlin, Lua или горутины в Go) мультиплексируются (распределяются) на один или несколько потоков ядра. Существует три основных модели: многие к одному (N:1), один к одному (1:1) и многие ко многим (M:N).

<img src="https://habrastorage.org/webt/ej/ku/8v/ejku8vs7jez-iqpxz2lzpawe7gm.png" />

_Многие к одному_
<br />

<img src="https://habrastorage.org/webt/qi/25/i3/qi25i3yxz24zeadcndn5tj7w574.png" />

_Один к одному_
<br />

<img src="https://habrastorage.org/webt/dr/hr/dr/drhrdru7lk5-61i5-rufzudbi6e.png" />

_Многие ко многим_
<br />

В Go используется модель M:N, позволяющая мультиплексировать несколько горутин на несколько потоков ядра. Такой подход, хоть и является более сложным, позволяет использовать преимущества многоядерных систем и повышает эффективность программ при выполнении системных вызовов, решая проблемы других моделей. Поскольку ядро не знает, что такое горутина, и предоставляет поток только в качестве единицы параллельного выполнения приложений пользовательского пространства, именно поток ядра выполняет логику планирования и код горутин, а также осуществляет системные вызовы от их имени.

На ранних этапах, особенно до версии 1.1, Go реализовывал модель многопоточности M:N наивным способом. Существовали только две сущности: горутины (`G`) и потоки ядра (`M`, _машины_). Для хранения всех запущенных горутин использовалась одна глобальная очередь выполнения (global run queue), защищенная блокировкой (lock) для предотвращения возникновения состояния гонки (race condition). Планировщик, работающий в каждом потоке, отвечал за извлечение горутины из глобальной очереди и ее выполнение.

<img src="https://habrastorage.org/webt/wt/kv/8u/wtkv8ug_chwjlla02uhm7-g6wng.png" />
<br />

Сегодня Go известен своей высокопроизводительной моделью параллельных вычислений. В ранних версиях это было совсем не так. Дмитрий Вьюков, один из ключевых разработчиков Go, указал на множество проблем с реализацией планировщика в работе ["Масштабируемый дизайн планировщика Go"](https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw): "В целом, планировщик может препятствовать пользователям использовать идиоматический мелкозернистый параллелизм там, где производительность имеет решающее значение". Позвольте объяснить, что он имел ввиду.

Во-первых, глобальная очередь выполнения являлась узким местом для производительности. При создании горутины, потокам приходилось получать блокировку, чтобы поместить ее в глобальную очередь. Аналогично, когда потоки хотели взять горутину из глобальной очереди, им также приходилось получать блокировку. Как известно, блокировка не бесплатна, она сопряжена с накладными расходами, связанными с конкуренцией за блокировку (lock contention). Конкуренция за блокировку приводит к снижению производительности, особенно в сценариях с высокой степенью параллелизма.

Во-вторых, потоки часто передают связанную с ними горутину другому потоку. Это приводит к плохой локальности и чрезмерным накладным расходам на переключение контекста. Дочерняя горутина обычно хочет взаимодействовать со своей родительской горутиной. Поэтому запуск дочерней горутины в том же потоке, что и родительской, обеспечивает более высокую производительность.

В-третьих, поскольку Go использует [кэширование потоков с помощью Malloc](https://google.github.io/tcmalloc/design.html) (Thread-caching Malloc), каждый поток имеет свой локальный кэш [mcache](https://nghiant3223.github.io/2025/06/03/memory_allocation_in_go.html#processors-memory-allocator-mcache), который он может использовать для выделения или удержания свободной памяти. Хотя `mcache` используется только потоками, выполняющими код Go, он также подключается к потокам, блокирующимся в системных вызовах, которые его вообще не используют. Кэш может занимать до 2 МБ памяти и не освобождается до тех пор, пока поток не будет уничтожен. Поскольку соотношение потоков, выполняющих код, и всех потоков может достигать 1:100 (слишком много потоков блокируются в системных вызовах), это может привести к чрезмерному потреблению ресурсов и плохой локальности данных.

## Улучшение планировщика

Рассмотрим некоторые предложения по улучшению планировщика, чтобы увидеть, как команда Go решила обозначенные проблемы, благодаря чему сегодня у нас есть высокопроизводительный планировщик.

### 1. Введение локальной очереди

Каждый поток `M` обладает локальной очередью выполнения для хранения выполняемых горутин. Когда выполняющаяся горутина `G` в потоке `M` запускает новую горутину `G1` с помощью ключевого слова `go`, горутина `G1` добавляется в локальную очередь выполнения потока `M`. Если локальная очередь заполнена, `G1` помещается в глобальную очередь. При выборе горутины для выполнения, `M` сначала проверяет свою локальную очередь, затем обращается к глобальной очереди. Таким образом, это предложение решает первую и вторую проблемы, описанные выше.

<img src="https://habrastorage.org/webt/if/me/df/ifmedf4xyra5hw3de_dm9nbtbwm.png" />
<br />

Однако это не решает третью проблему. Когда множество потоков блокируются в системных вызовах, [mcache](https://nghiant3223.github.io/2025/06/03/memory_allocation_in_go.html#processors-memory-allocator-mcache) остаются подключенными, что приводит к высокому потреблению памяти самим планировщиком, не говоря уже о потреблении памяти нашей программой.

Это также создает еще одну проблему с производительностью. Чтобы избежать "голодания" (starving) горутин в локальной очереди заблокированного потока, как `M1` на рисунке выше, планировщик должен разрешить другим потокам _забирать_ (steal - красть) горутины из него. Однако при большом количестве заблокированных потоков, сканирование их всех для поиска непустой очереди выполнения становится дорогим.

### 2. Внедрение логического процессора

Это предложение описано в книге ["Масштабируемый дизайн планировщика Go"](https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw), где вводится понятие _логического процессора_ (logical processor) `P`. _Логический_ означает, что `P` притворяется выполняющим код, хотя на самом деле это осуществляет связанный с ним поток `M`. Локальная очередь потока и [mcache](https://nghiant3223.github.io/2025/06/03/memory_allocation_in_go.html#processors-memory-allocator-mcache) теперь принадлежат `P`.

Это эффективно решает третью проблему. Поскольку [mcache](https://nghiant3223.github.io/2025/06/03/memory_allocation_in_go.html#processors-memory-allocator-mcache) теперь привязан к `P`, а не к `M`, а `M` отсоединен от `P`, когда `G` выполняет системный вызов, потребление памяти остается низким при большом количестве `M`, выполняющих системные вызовы. Также, поскольку количество `P` ограничено, механизм перехвата (stealing) горутин является эффективным.

<img src="https://habrastorage.org/webt/ks/bd/wk/ksbdwkjpqgnq7jji0sk_ibtbs68.png" />
<br />

С появлением логических процессоров многопоточная модель M:N в Go стала именоваться моделью GMP, поскольку в ней задействовано три типа сущностей: горутина, поток и процессор.

## Модель GMP

### Горутина [g](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L396-L508)

Когда за ключевым словом `go` следует вызов функции, создается новый экземпляр [g](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L396-L508), именуемый `G`. `G` - это объект, представляющий горутину, содержащий метаданные, такие как состояние выполнения, стек и программный счетчик (program counter), указывающий на соответствующую функцию. Таким образом, выполнение горутины означает запуск функции, на которую ссылается `G`.

Когда горутина выполняется, она не уничтожается. Она становится _мертвой_ (dead) и помещается в свободный список текущего процессора `P`. Если свободный список `P` полон, мертвая горутина помещается в глобальный свободный список. При создании новой горутины планировщик пытается повторно использовать одну из свободного списка, вместо создания новой с нуля. Этот механизм значительно удешевляет создание горутин.

Рисунок и таблица ниже описывают жизненный цикл горутин в модели GMP. Некоторые состояния и переходы опущены для простоты. Операции переходов будут описаны позже.

| Состояние                                                                              | Описание                                                                |
|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------|
| [Idle](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L36-L39)     | Только что создана и еще не инициализирована                            |
| [Runnable](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L40-L42) | Находится в очереди на выполнение и готова к запуску                    |
| [Running](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L44-L47)  | Не находится в очереди и в данный момент выполняет код                  |
| [Syscall](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L49-L52)  | Выполняет системный вызов и не исполняет пользовательский код           |
| [Waiting](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L54-L62)  | Не выполняет код и не находится в очереди, например, ждет канал         |
| [Dead](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L68-L74)     | Находится в свободном списке, только что завершена или инициализируется |

<img src="https://habrastorage.org/webt/mn/6d/oi/mn6doioduuhc9n1mubxraif5qzg.png" />
<br />

### Поток [m](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L528-L630)

Весь код Go, будь то пользовательский код, планировщик или сборщик мусора, выполняется в потоках, управляемых ядром ОС. Для повышения эффективности работы планировщика в модели GMP вводится структура [m](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L528-L630), представляющая экземпляр потока `M`.

`M` содержит ссылку на текущую горутину `G`, текущий процессор `P`, если `M` выполняет код, предыдущий процессор `P`, если `M` выполняет системный вызов, и следующий процессор `P`, если `M` создается.

Каждый `M` также содержит ссылку на специальную горутину `g0`, которая запускается в системной стеке (system stack) - стеке, предоставляемом потоку ядром. В отличие от системного стека, обычный стек горутины имеет динамический размер, который увеличивается и уменьшается при необходимости. Однако, операции увеличения и уменьшения стека сами должны запускаться в валидном стеке. Для этого используется системный стек. Когда планировщику, запущенному в `M`, требуется управление стеком, он переключается со стека горутины на системный стек. Такие операции, как сборка мусора и [парковка горутины](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#goroutine-parking-gopark), также выполняются в системной стеке. При выполнении такой операции, планировщик переключается на системный стек и выполняет операцию в контексте `g0`.

В отличие от горутин, потоки запускают код планировщика `M` при создании, поэтому начальным состоянием `M` является _running_. Когда `M` создается или пробуждается, планировщик всегда гарантирует наличие процессора `P` в состоянии _idle_, который может быть связан с `M` для выполнения кода. Когда `M` выполняет системный вызов, он отсоединяется от `P`, который может использоваться другим потоком `M1` для продолжения его работы. Если `M` не может найти горутину в состоянии _runnable_ ни в локальной, ни в глобальной очереди (`netpoll`), он зацикливается (spinning - вращается), пытаясь забрать горутину у другого процессора `P`, а затем снова заглядывает в глобальную очередь. Обратите внимание, что не все `M` зацикливаются, это происходит только в случае, когда количество таких потоков меньше половины занятых процессоров. Когда `M` нечем заняться, он засыпает и ждет, когда его возьмет другой процессор `P1`.

Рисунок и таблица ниже описывают жизненный цикл потоков в модели GMP. Некоторые состояния и переходы опущены для простоты. _Spinning_ - это вид _idle_, когда поток потребляет циклы центрального процессора (ЦП, CPU) для выполнения кода среды выполнения, который "заимствует" горутину. Операции переходов будут описаны позже.

| Состояние | Описание                                                    |
|-----------|-------------------------------------------------------------|
| Running   | Выполняет код среды выполнения или пользовательский код     |
| Syscall   | В данный момент выполняет системный вызов (заблокирован)    |
| Spinning  | Берет (крадет) горутину у других процессоров                |
| Sleep     | Находится в состоянии сна, не потребляет ресурсы процессора |

<img src="https://habrastorage.org/webt/na/oa/l3/naoal30targa_tra_hiv3soezri.png" />
<br />

### Процессор [p](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L632-L757)

Структура [p](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L632-L757) концептуально представляет физический процессор для выполнения горутин. Экземпляры [p](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L632-L757) называются `P` и создаются на этапе начальной загрузки (bootstrap) программы. Хотя количество создаваемых потоков может быть большим ([10 000](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L827-L827) в Go 1.24), количество процессов обычно является небольшим и определяется [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS). Существует ровно [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) процессоров, независимо от их состояния.

Для минимизации конфликтов блокировок в глобальной очереди выполнения, каждый процессор `P` в среде выполнения Go поддерживает локальную очередь выполнения. Локальная очередь на самом деле состоит из двух компонентов: [runnext](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L655-L667), содержащий одну приоритетную горутину, и [runq](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L654-L654) - очередь горутин. Оба компонента выступают источником _runnable_ горутин для `P`, но `runnext` существует исключительно как оптимизация производительности. Планировщик позволяет `P` брать горутины из локальной очереди другого процессора `P1`. `runnext` процессора `P1` исследуется только в том случае, если первые три попытки заимствовать горутину из `runq` были неудачными. Поэтому, когда `P` хочет выполнить горутину, возникает меньше конфликтов блокировок, если он сначала обращается к своему `runnext`.

Компонент `runq` - это массив фиксированного размера с циклической структурой. Массив фиксированного размера с 256 слотами обеспечивает лучшую локальность кэша и снижает накладные расходы на выделение памяти. Фиксированный размер безопасен для локальных очередей `P`, поскольку у нас также есть глобальная очередь в качестве резервной. Циклическая структура позволяет эффективно добавлять и удалять горутины без необходимости перемещения элементов.

[mcache](https://nghiant3223.github.io/2025/06/03/memory_allocation_in_go.html#processors-memory-allocator-mcache) служит интерфейсом [модели кэширования потоков Malloc](https://google.github.io/tcmalloc/design.html) и используется `P` для выделения микро (micro) и маленьких (small) объектов. [pageCache](https://github.com/golang/go/blob/go1.24.0/src/runtime/mpagecache.go#L14-L22), с другой стороны, позволяет распределителю (allocator) получать страницы памяти без получения [блокировки кучи](https://www.ibm.com/docs/en/sdk-java-technology/8?topic=management-heap-allocation#the-allocator), что повышает производительность при высоком параллелизме.

Для корректной работы программы с [задержками](https://pkg.go.dev/time#Sleep) (sleep), [таймаутами](https://pkg.go.dev/time#After) и [интервалами](https://pkg.go.dev/time#Tick), `P` также управляет таймерами, реализованными с помощью [min-heap](https://en.wikipedia.org/wiki/Heap_(data_structure)), где ближайший таймер находится на вершине кучи. При поиске готовой к запуску горутины, `P` также проверяет наличие истекших таймеров. Если таковые имеются, `P` добавляет соответствующую горутину в свою локальную очередь, что она могла быть запущена.

Рисунок и таблица ниже описывают жизненный цикл процессоров в модели GMP. Некоторые состояния и переходы опущены для простоты. Операции переходов будут описаны позже.

| Состояние                                                                               | Описание                                                                                                               |
|-----------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|
| [Idle](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L113-L120)    | Не выполняет код среды выполнения или пользовательский код                                                             |
| [Running](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L122-L129) | Связан с `M`, который выполняет пользовательский код                                                                   |
| [Syscall](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L131-L141) | Связан с `M`, который выполняет системный вызов                                                                        |
| [GCStop](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L143-L151)  | Связан с `M`, который "остановил мир" (stop-the-world) для сборки мусора                                               |
| [Dead](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L153-L157)    | Больше не используется, ожидает повторного использования при росте [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) |

<img src="https://habrastorage.org/webt/1w/6f/q5/1w6fq5iwoazlcfwxulhqub-kfkg.png" />
<br />

В начале выполнения программы количество процессоров в состоянии _idle_ равно [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS). Когда поток `M` берет процессор для выполнения пользовательского кода, `P` переходит в состояние _running_. Если текущая горутина `G` выполняет системный вызов, `P` отсоединяется от `M` и переходит в состояние _syscall_. Во время системного вызова, если `P` захватывается `sysmon` (см. раздел "Некооперативное вытеснение" ниже), он сначала переходит в состояние _idle_, затем передается другому потоку `M1` и переходит в состояние _running_. Иначе, после завершения системного вызова, `P` подключается к последнему `M` и продолжает выполняться (см. раздел "Обработка системных вызовов" ниже). При сборке мусора `P` переходит в состояние _gcStop_ и возвращается в предыдущее состояния после завершения сборки. Если [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) уменьшается во время выполнения, лишние процессоры переходят в состояние _dead_ и повторно используются, если [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) позже увеличивается.

## Начальная загрузка программы

Для включения планировщика Go, он должен быть инициализирован при запуске программы. Эта инициализация выполняется в ассемблере с помощью функции [runtime.rt0_go](https://github.com/golang/go/blob/go1.24.0/src/runtime/asm_amd64.s#L159-L159). На этом этапе создаются поток [M0](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L117-L117) (основной поток) и горутина [G0](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L118-L118) (горутина системного стека [M0](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L117-L117)). Также настраивается [локальное хранилище потока](https://en.wikipedia.org/wiki/Thread-local_storage) (thread-local storage, TLS) для основного потока, где сохраняется адрес [G0](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L118-L118), что позволяет позже получить к нему доступ с помощью функции [getg](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#getting-goroutine-getg).

Затем вызывается функция ассемблера [runtime.schedinit](https://github.com/golang/go/blob/go1.24.0/src/runtime/asm_amd64.s#L349), реализацию которой на Go можно найти [здесь](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L790-L898). Эта функция выполняет несколько инициализаций, в частности, вызывает функцию [procresize](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L5719-L5868), которая устанавливает [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) логических процессоров `P` в состояние _idle_. Основной поток [M0](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L117-L117) подключается к первому процессору, переводя его из состояния _idle_ в состояние _running_ для выполнения горутин.

Затем создается основная горутина для запуска функции [runtime.main](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L146-L148), которая служит входной точкой среды выполнения Go. Внутри [runtime.main](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L146-L148) создается отдельный поток для запуска `sysmon` (см. раздел "Некооперативное вытеснение" ниже). Обратите внимание, что [runtime.main](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L146-L148) - это не функция `main`, которую мы пишем. Последняя появляется в среде выполнения как [main_main](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L134-L135).

Далее, основной поток вызывает [mstart](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L1769-L1769) для начала выполнения на [M0](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L117-L117), запуская [цикл планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop), который берет и выполняет основную горутину. В [runtime.main](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L146-L148), после дополнительных шагов инициализации, управление, наконец, передается пользовательской функции [main_main](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L134-L135), и программа начинает выполнять пользовательский код.

Следует отметить, что основной поток [M0](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L117-L117) отвечает не только за запуск основной горутины, но также за выполнение других горутин. Как только основная горутина блокируется, например, ожидая системного вызова или канал, основной поток ищет другую готовую к выполнению горутину и выполняет ее.

Суммируя, при запуске программы есть:

- одна горутина `G`, выполняющая функцию `main`
- два потока - один основной `M0` и другой для запуска `sysmon`
- один процессор `P0` в состоянии _running_ и `GOMAXPROCS-1` процессоров в состоянии _idle_

Основной поток `M0` связывается с процессором `P0` для запуска основной горутины `G`.

Рисунок ниже демонстрирует состояние программы при запуске. Предполагается, что [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) равняется `2` и только что запущена функция `main`. Процессор `P0` выполняет основную горутину и поэтому находится в состоянии _running_. Процессор `P1` не выполняет горутин, поэтому находится в состоянии _idle_. Хотя основной поток `M0` связан с процессором `P0` для выполнения основной горутины, другой поток `M1` создан для запуска `sysmon`.

<img src="https://habrastorage.org/webt/t0/9o/lc/t09olcocrj4ee6uh39dacswvmew.png" />
<br />

Следует отметить, что на этапе начальной загрузки среда выполнения также запускает несколько других горутин, связанных с управлением памятью, таких как маркировка, очистка и освобождение памяти. Однако мы не будем рассматривать их в рамках данной статьи.

## Создание горутины

Go предоставляет простой API для запуска параллельного вычисления: `go func() { ... } ()`. Под капотом среда выполнения Go выполняет множество сложных задач для этого. Ключевое слово `go` - это просто синтаксический сахар для функции среды выполнения [newproc](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L5014-L5030), которая отвечает за планирование создания новой горутины. Это функция делает три вещи: инициализирует горутину, помещает ее в очередь выполнения процессора `P`, на котором выполняется горутина вызывающего, пробуждает другой процессор `P1`.

### Инициализация горутины

При вызове [newproc](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L5014-L5030) новая горутина `G` создается только в случае, когда нет горутин в состоянии _idle_. Горутины переходят в это состояние после возврата из выполнения. Новая горутина `G` инициализируется 2 КБ стеком, как определено в константе среды выполнения [stackMin](https://github.com/golang/go/blob/go1.24.0/src/runtime/stack.go#L75-L75). Кроме того, функция [goexit](https://github.com/golang/go/blob/go1.24.0/src/runtime/stubs.go#L281-L291), отвечающая за логику очистки и планирования, помещается в стек вызовов `G` для обеспечения ее вызова после возврата `G`. После инициализации `G` переходит из состояния _dead_ в состояние _runnable_, что указывает на ее готовность к планированию для выполнения.

### Помещение горутины в очередь

Как упоминалось ранее, каждый процессор `P` имеет очередь выполнения, состояющую из двух частей: [runnext](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L655-L667) и [runq](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L654-L654). Новая горутина помещается в [runnext](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L655-L667) при создании. Если в [runnext](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L655-L667) уже есть горутина `G1`, планировщик пытается переместить `G1` в [runq](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L654-L654) и, если удалось, помещает `G` в [runnext](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L655-L667). Если [runq](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L654-L654) полна, `G1` вместе с половиной горутин из [runq](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L654-L654) помещается в глобальную очередь выполнения для уменьшения рабочей нагрузки `P`.

### Пробуждение процессора

При создании новой горутины, если мы стремимся к максимизации параллелизма программы, поток, на котором выполняется горутина, будит другой процессор `P` с помощью системного вызова [futex](https://man7.org/linux/man-pages/man2/futex.2.html). Сначала проверяется наличие процессоров в состоянии _idle_. Если такой процессор доступен, либо создается новый поток, либо пробуждается существующий для входа в [цикл планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop), где он будет искать готовую к выполнению горутину.

Как уже упоминалось, [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) (количество активных процессоров `P`) определяет, сколько горутин может выполняться одновременно. Если все процессоры заняты и постоянно появляются новые горутины, ни существующий поток не пробуждается, ни новый не создается.

### Резюме

На рисунке ниже показан процесс создания горутин. Для простоты предполагается, что [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) равняется `2`, процессор `P1` еще не вошел в [цикл планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop), а функция `main` только создает новые горутины. Поскольку горутины не выполняют системные вызовы, создается ровно один дополнительный поток `M2` для соединения с процессором `P1`.

<img src="https://habrastorage.org/webt/3i/nd/c6/3indc6twwxpcqsxikqykbjmdckk.png" />
<br />

## Цикл планирования

Функция [schedule](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3986-L4068) среды выполнения Go отвечает за поиск и выполнения _runnable_ горутины. Она вызывается в различных сценариях: при создании нового потока, при вызове [Gosched](https://pkg.go.dev/runtime#Gosched), при парковке или вытеснении горутины, а также после выполнения системного вызова и возврата горутины.

Процесс выбора готовой к выполнению горутины сложен и будет подробно рассмотрен в отдельном разделе. После выбора, горутина переходит из состояния _runnable_ в состояние _running_, что свидетельствует о ее готовности к запуску. В этот момент поток ядра вызывает функцию [gogo](https://github.com/golang/go/blob/go1.24.0/src/runtime/stubs.go#L214-L214) для начала выполнения горутины.

Но почему это называется циклом? Как описывалось в разделе "Инициализация горутины", после завершения горутины вызывается функция [goexit](https://github.com/golang/go/blob/go1.24.0/src/runtime/stubs.go#L281-L291). Эта функция в конечном итоге приводит к вызову функции [goexit0](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4307-L4310), которая выполняет очистку для завершившейся горутины и потворно входит в функцию [schedule](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3986-L4068), возвращая [цикл планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop) в исходное состояние.

Следующая диаграмма иллюстрирует цикл планирования в среде выполнения Go, где розовые блоки относятся к пользовательскому коду, а желтые - к коду среды выполнения. Это может казаться очевидным, но обратите внимание, что цикл планирования выполняется потоком. Вот почему это происходит после инициализации потока (голубой блок).

<img src="https://habrastorage.org/webt/lw/ef/eu/lwefeu9bm-nk34bkhkvo25moon8.png" />
<br />

Но если основной поток застрял в цикле планирования, как процесс может завершиться? Взгляните на функцию [main](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L307-L307) в среде выполнения, которая выполняется основной горутиной. После возврата [main_main](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L134-L135) (синоним пользовательской функции `main`), выполняется системный вызов [exit](https://man7.org/linux/man-pages/man3/exit.3.html) для завершения процесса. Вот как процесс может быть завершен и вот почему основная горутина не ждет горутины, созданные с помощью ключевого слова `go`.

## Поиск готовой к выполнения горутины

Задача потока `M` - найти подходящую готовую к выполнению горутину, чтобы свести к минимуму "голодание" горутин. Эта логика реализована в функции [findRunnable](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3267-L3646), которая вызывается [циклом планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop).

Поток `M` ищет готовую к выполнению горутину в следующем порядке:

1. Проверяет доступность горутины [для чтения трассировки](https://go.dev/blog/execution-traces-2024#trace-reader-api) (см. раздел "Некооперативное вытеснение").
2. Проверяет доступность рабочей горутины сборки мусора (см. раздел "Сборщик мусора").
3. Один раз в 1/61 времени проверяет глобальную очередь выполнения.
4. Проверяет локальную очередь выполнения связанного процессора `P`, если `M` зациклен (spinning).
5. Снова проверяет глобальную очередь.
6. Проверяет наличие горутины, готовой к вводу-выводу, в `netpoll` (см. раздел "Работа netpoll").
7. Заимствует горутину из локальной очереди другого процессора.
8. Снова проверяет доступность рабочей горутины сборки мусора.
9. Снова проверяет глобальную очередь, если `M` зациклен.

Шаги 1, 2 и 8 предназначены только для внутреннего использования среды выполнения Go. На шаге 1 горутина для чтения трассировки используется для отслеживания выполнения программы. Мы поговорим об этом позже. Между тем, шаги 2 и 8 позволяют сборщику мусора работать параллельно с обычными горутинами. Хотя эти шаги не влияют на видимый пользователю прогресс, они необходимы для корректной работы среды выполнения.

Шаги 3, 5 и 9 не ограничиваются одной горутиной, но пытаются взять сразу группу (batch) для повышения эффективности. Размер группы рассчитывается как `(размер_глобальной_очереди/количество_процессоров)+1`, но ограничен несколькими факторами: не может быть превышен определенный параметр максимума и не может быть занято больше половины емкости локальной очереди `P`. После определения размера группы, одна горутина запускается сразу, другие помещаются в локальную очередь `P`. Такой подход позволяет распределить нагрузку между процессорами и снижает конкуренцию за блокировку глобальной очереди, поскольку процессоры реже к ней обращаются.

Шаг 4 немного сложнее, поскольку локальная очередь `P` состоиз из двух частей: `runnext` и `runq`. Сначала проверяется `runnext`. Если там есть горутина, она возвращается. Если `runnext` пуст, проверяется `runq`. Шаг 6 будет подробно описан в разделе "Работа netpoll".

Шаг 7 - самая сложная часть процесса. Предпринимается до четырех попыток взять горутины у другого процессора `P1`. Первые три раза проверяется `runq`. Если она не пуста, половина горутин из `runq` процессора `P1`  перемещается в `runq` текущего процессора `P`. В четвертый раз проверяется `runnext` `P1`, а затем снова `runq` `P1`.

Обратите внимание, что функция [findRunnable](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3267-L3646) не только ищет готовые к выполнению горутины, но также будит горутины, ушедшие спать до шага 1. После пробуждения, горутина помещается в локальную очередь процессора `P`, который ее выполнял, ожидая выполнения некоторым потоком `M`.

Если после шага 9 горутины не найдено, поток `M` ждет истечения ближайшего [таймера](https://github.com/golang/go/blob/go1.24.0/src/runtime/time.go#L35-L107) в `netpoll`, например, пробуждения горутины (при переводе в спящий режим создается таймер). Почему `netpoll` связан с таймерами? Система таймеров Go сильно зависит от `netpoll`, как указано в [этом комментарии к коду](https://github.com/golang/go/blob/go1.24.0/src/runtime/time.go#L427-L427). После возврата `netpoll`, `M` повторно входит в [цикл планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop) для поиска готовой к выполнению горутины.

Предыдущие два варианта поведения [findRunnable](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3267-L3646) позволяют планировщику Go пробуждать спящие горутины, позволяя программе продолжать выполнение. Рассмотрим работу следующей программы:

```go
package main

import "time"

func main() {
    go func() {
        time.Sleep(time.Second)
    }()

    time.Sleep(2*time.Second)
}
```

Если у `P` нет [таймера](https://github.com/golang/go/blob/go1.24.0/src/runtime/time.go#L35-L107), поток `M` будет свободен (idle). `P` помещается в свободный список, `M` отправляется спать путем вызова функции [stopm](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#stop-thread-stopm). Он будет спать, пока другой поток `M1` не разбудит его, как правило, во время создания новой горутины. После пробуждения `M` повторно входит в [цикл планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop) для поиска и выполнения _runnable_ горутины.

## Вытеснение горутин

Вытеснение (preemption) - это временное прерывание выполнения горутины для запуска других горутин для предотвращения их "голодания". Существует два типа вытеснения:

- некооперативное (non-ciooperative) - слишком долго выполняющаяся горутина принудительно останавливается
- кооперативное (cooperative) - горутина добровольно уступает (yield) свой процессор

### Некооперативное вытеснение

Рассмотрим пример некооперативного выстеснения. У нас имеется две горутины, которые вычисляют число Фибоначчи, что представляет собой замкнутый цикл с ресурсоемкими операциями на ЦП. Для обеспечения одновременного выполнения только одной горутины максимальное количество логических процессоров [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) устанавливается равным `1` при запуске программы: `GOMAXPROCS=1 go run main.go`.

```go
package main

import (
    "runtime"
    "time"
)

func fibonacci(n int) int {
    if n <= 1 {
        return n
    }
    previous, current := 0, 1
    for i := 2; i <= n; i++ {
        previous, current = current, previous+current
    }
    return current
}

func main() {
    go fibonacci(1_000_000_000)
    go fibonacci(2_000_000_000)

    time.Sleep(3*time.Second)
}
```

У нас имеется только один процессор `P`. Что произойдет? Варианты:

- ни одна горутина не запускается, поскольку основная функция "захватила" `P`
- одна горутина выполняется, другая голодает
- обе горутины выполняются одновременно (в данном случае конкурентно) почти магически

К счастью, Go позволяет легко понять, что происходит в планировщике. Пакет [runtime/trace](https://go.dev/pkg/runtime/trace) содержит мощный инструмент для поиска и устранения проблем Go-программ. Для его использования необходимо добавить в метод `main` экспорт трассировки в файл:

```go
func main() {
    file, _ := os.Create("trace.out")
    _ = trace.Start(file)
    defer trace.Stop()
    ...
}
```

После завершения программы используем команду `go tool trace trace.out` для визуализации трассировки. Мой файл `trace.out` можно найти [здесь](https://nghiant3223.github.io/assets/2025-03-11-go-scheduling/non_cooperative_preempt_trace.out). На рисунке ниже горизонтальная ось показывает, какая горутина выполняется на `P` в определенное время. Ожидаемо существует только один логический процессор `Proc 0`.

<img src="https://habrastorage.org/webt/qy/6r/fi/qy6rfipjrtfmbjbheghzo3jbqhe.png" />
<br />

Увеличив масштаб (клавиша "W") в начале линии времени, можно увидеть, что процесс начинается с `main.main` (функция `main` в пакете `main`), которая выполняется в основной горутине `G1`. Через несколько миллисекунд также на `Proc 0` горутина `G10` планируется для выполнения функции `fibonacci`, захватывая процессор и вытесняя `G1`.

<img src="https://habrastorage.org/webt/1z/5v/ok/1z5vokor8m4hucaiwhd8ybevz_0.png" />
<br />

Уменьшив масштаб (клавиша "S") и прокрутив немного вправо, можно увидеть, что `G10` позже заменяется другой горутиной `G9`, следующим экземпляром, выполняющим функцию `fibonacci`. Эта горутина также выполняется на `Proc 0`. Запомните `runtime.asyncPreempt:47`, мы вернемся к этому чуть позже.

Таким образом, Go способен прерывать горутины, которые интенсивно используют ресурсы процессора. Но почему это возможно? Ведь если горутина постоянно потребляет все ресурсы процессора, как ее можно прервать? Это сложная проблема, и она долго [обсуждалась](https://github.com/golang/go/issues/10958) в трекере ошибок Go. Проблема не решалась до версии Go 1.14, в которой было представлено асинхронное прерывание.

В среде выполнения Go есть демон, "живущий" в отдельном потоке без процессора - `sysmon` (от "system monitor" - анализатор системы). Когда `sysmon` находит горутину, которая ипользует `P` дольше 10 мс (константа среды выполнения [forcePreemptNS](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L6245-L6245)), он через системный вызов [tgkill](https://man7.org/linux/man-pages/man2/tkill.2.html) указывает потоку `M` принудительно выстеснить выполняющуюся горутину. Да, вы правильно прочитали. Согласно [странице руководства Linux](https://man7.org/linux/man-pages/man2/tkill.2.html), [tgkill](https://man7.org/linux/man-pages/man2/tkill.2.html) используется для отправки потоку сигнала, а не для его "убийства". Сигналом является `SIGURG`, причина выбора объясняется [здесь](https://github.com/golang/go/blob/go1.24.0/src/runtime/signal_unix.go#L43-L73).

После получения `SIGURG` выполнение программы передается обработчику сигнала, регистрируемому вызовом функции [initsig](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L1879-L1879) при инициализации потока. Обратите внимание, что обработчик сигнала может выполняться параллельно с кодом горутины или кодом планировщика, как показано на рисунке ниже. Переключение выполнения с основной программы на обработчик сигнала выполняется ядром.

<img src="https://habrastorage.org/webt/fy/gi/dr/fygidrsx3qiafjnme6iccgap5xs.png" />
<br />

В обработчике сигнала счетчик программ устанавливается на функцию [asyncPreempt](https://github.com/golang/go/blob/go1.24.0/src/runtime/preempt.go#L295-L299), позволяя приостановить выполнение горутины и создать пространство для вытеснения. В ассемблерном коде функции [asyncPreempt](https://github.com/golang/go/blob/go1.24.0/src/runtime/preempt_arm64.s) сохраняются регистры горутины и вызывается функция [asyncPreempt2](https://github.com/golang/go/blob/go1.24.0/src/runtime/preempt.go#L302-L311) на строке [47](https://github.com/golang/go/blob/go1.24.0/src/runtime/preempt_arm64.s#L47). Вот откуда берется `runtime.asyncPreempt:47` на визуализации. В [asyncPreempt2](https://github.com/golang/go/blob/go1.24.0/src/runtime/preempt.go#L302-L311) горутина `g0` потока `M` входит в [gopreempt_m](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4191-L4193) для отсоединения `G` от `M` и помещения `G` в глобальную очередь выполнения. Поток продолжает цикл планирования, находит другую горутину и выполняет ее.

Поскольку сигнал вытеснения инициируется `sysmon`, но фактическое вытеснение происходит после получения сигнала потоком, этот вид вытеснения является асинхронным. Вот почему горутины могут выполняться дольше 10 мс, как `G9` в примере.

<img src="https://habrastorage.org/webt/gw/mk/tp/gwmktpuig4zxn264-fhmjsreih4.png" />
<br />

### Кооперативное вытеснение в ранних версиях Go

В ранних версиях Go среда выполнения сама не могла вытеснять горутины с плотными циклами, как в примере выше. Программистам приходилось указывать горутинам кооперативно отказываться от своего процессора путем вызова функции [runtime.Gosched](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L358-L365) в теле цикла. На Stackoverflow есть [пример и описание поведения `runtime.Gosched`](https://stackoverflow.com/questions/13107958/what-exactly-does-runtime-gosched-do).

С точки зрения программиста это было очень утомительным и подверженным ошибкам, а также приводило к [некоторым проблемам с производительностью](https://github.com/golang/go/issues/12553). Поэтому команда Go решила реализовать умный способ вытеснения горутины самой средой выполнения.

### Кооперативное вытеснение в Go 1.14+

Вам интересно, почему я не использовал `fmt.Printf` для проверки запуска каждой горутины в терминале? Потому что вытеснение стало бы кооперативным.

#### Разбор программы

Скомпилируем программу и проанализируем ее ассемблерный код. Поскольку компилятор Go применяет различные оптимизации, которые могут затруднить отладку, необходимо отключить их при сборке программы. Это можно сделать с помощью команды `go build -gcflags="all=-N -l" -o fibonacci main.go`.

Для разбора (disassembling) функции `fibonacci` я использую [Delve](https://github.com/go-delve/delve), мощный отладчик для Go: `dlv exec ./fibonacci`. Находясь в отладчике, выполняем следующую команду для отображения ассемблерного кода `fibonacci`: `disassemble -l main.fibonacci`. Ассемблерный код моей программы можно найти [здесь](https://nghiant3223.github.io/assets/2025-03-11-go-scheduling/non_cooperative_preempt.s). Поскольку я компилировал программу на `darwin/arm64`, ваш ассемблерный код может отличаться от моего.

```
main.go:11      0x1023e8890     900b40f9        MOVD 16(R28), R16
main.go:11      0x1023e8894     f1c300d1        SUB $48, RSP, R17
main.go:11      0x1023e8898     3f0210eb        CMP R16, R17
main.go:11      0x1023e889c     090c0054        BLS 96(PC)
...
main.go:17      0x1023e8910     6078fd97        CALL runtime.convT64(SB)
...
main.go:17      0x1023e895c     4d78fd97        CALL runtime.convT64(SB)
...
main.go:20      0x1023e8a18     c0035fd6        RET
main.go:11      0x1023e8a1c     e00700f9        MOVD R0, 8(RSP)
main.go:11      0x1023e8a20     e3031eaa        MOVD R30, R3
main.go:11      0x1023e8a24     dbe7fe97        CALL runtime.morestack_noctxt(SB)
main.go:11      0x1023e8a28     e00740f9        MOVD 8(RSP), R0
main.go:11      0x1023e8a2c     99ffff17        JMP main.fibonacci(SB)
```

`MOVD 16(R28), R16` загружает значение со смещением (offset) `16` из регистра `R28`, содержащего структуру данных горутины [g](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L396-L396), и сохраняет это значение в регистре `R16`. Загруженное значение - это поле [stackguard0](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L405-L405), служащее защитой стека (stack guard) для текущей горутины. Но что такое защита стека? Возможно, вы знаете, что стек горутины может увеличиваться в размерах (growable), но как среда выполнения Go понимает, что пора это сделать? Защита стека — это специальное значение, помещаемое в конец стека. Когда указатель стека достигает этого значения, среда выполнения понимает, что стек почти заполнен и нуждается в увеличении — именно это и делают следующие три инструкции.

`SUB $48, RSP, R17` загружает указатель стека горутины из регистра `RPS` в регистр `R17` и вычитает из него `48`. `CMP R16, R17` сравнивает защиту стека с указателем стека, а `BLS 96(PC)` переходит к инструкции, находящейся на 96 инструкций впереди программы, если указатель стека меньше или равен защите стека. Почем меньше или равен (≤), а не больше или равен (≥)? Поскольку стек растет вниз, указатель стека всегда больше защиты стека.

Почему эти инструкции не отображаются в коде Go, но присутствуют в ассемблерном коде? Это происходит потому, что при компиляции компилятор Go автоматически вставляет их в [пролог](https://en.wikipedia.org/wiki/Function_prologue_and_epilogue) функции. Это применяется ко всем функциям, вроде `fmt.Println`, а не только к нашей `fibonacci`.

Через 96 инструкций выполнение достигает инструкции `MOVD R0, 8(RSP)`, затем переходит к `CALL runtime.morestack_noctxt(SB)`. Функция [runtime.morestack_noctxt](https://github.com/golang/go/blob/go1.24.0/src/runtime/asm_arm64.s#L348-L348) в конечном счете вызывает [newstack](https://github.com/golang/go/blob/go1.24.0/src/runtime/stack.go#L966-L966) для увеличения стека и опционально входит в [gopreempt_m](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4191-L4193) для запуска некооперативного вытеснения. Ключевым для кооперативного вытеснения является условие входа в `gopreempt_m` - [stackguard0 == stackPreempt](https://github.com/golang/go/blob/go1.24.0/src/runtime/stack.go#L1025-L1025). Это означает, что горутина, желающая увеличить стек, будет вытеснена, если ее `stackguard0` ранее был установлен в [stackPreempt](https://github.com/golang/go/blob/go1.24.0/src/runtime/stack.go#L128-L130).

[stackPreempt](https://github.com/golang/go/blob/go1.24.0/src/runtime/stack.go#L128-L130) может быть установлен `sysmon`, если горутина выполняется дольше 10 мс. Затем горутина будет вытеснена кооперативно при вызове функции или некооперативно обработчиком сигналов (signal handler) потока, в зависимости от того, что произойдет раньше. Он также может быть установлен, когда горутина входит/выходит из системного вызова или на этапе трассировки сборщика мусора. См. [вытеснение sysmon](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L6366-L6366), [вход](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4525-L4525)/[выход syscall](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4663-L4663), [трассировка сборщика мусора](https://github.com/golang/go/blob/go1.24.0/src/runtime/trace.go#L389-L389).

#### Визуализация трассировки

Возвращаемся к программе, убеждаемся, что `GOMAXPROCS=1`, и изучаем трассировку.

<img src="https://habrastorage.org/webt/9o/7d/a7/9o7da7vmmrbbs7gsibym58rjsya.png" />
<br />

Четко видно, что горутины освобождают логический процессор всего через несколько десятков микросекунд, в отличие от некооперативного вытеснения, где они могут удерживать его более 10 мс. Примечательно, что трассировка стека `G9` заканчивается на `fmt.Printf` внутри тела цикла, демонстрируя проверку защиты стека в прологе функции. Эта визуализация точно иллюстрирует кооперативное вытеснение, при котором горутины _добровольно_ уступают свой процессор.

<img src="https://habrastorage.org/webt/h5/05/7f/h5057furr3t5rqfmm-tnue6iqnw.png" />
<br />

## Обработка системных вызовов

[Системные вызовы](https://en.wikipedia.org/wiki/System_call) - это службы, предоставляемые ядром, доступные пользовательским программам через API. Эти службы включают фундаментальные операции, например, чтение файлов, установку соединений или выделение памяти. В Go редко требуется обращаться к системным вызовам напрямую, стандартная библиотека предоставляет высокоуровневые абстракции для упрощения этих задач.

Однако, понимание работы системных вызовов является критическим для погружения в среду выполнения Go, внутреннего устройства стандартной библиотеки и оптимизации производительности. Среда выполнения использует модель потоков M:N, оптимизированную логическими процессорами, что делает обработку системных вызовов очень интересной.

### Классификация системных вызовов

В среде выполнения существует две обертки над системными вызовами: [RawSyscall](https://github.com/golang/go/blob/go1.24.0/src/syscall/syscall_linux.go#L54-L56) и [Syscall](https://github.com/golang/go/blob/go1.24.0/src/syscall/syscall_linux.go#L72-L89). Код Go, который мы пишем, использует эти функции для выполнения системных вызовов. Каждая функция принимает номер системного вызова, его аргументы и возвращает значения и код ошибки.

[Syscall](https://github.com/golang/go/blob/go1.24.0/src/syscall/syscall_linux.go#L72-L89) обычно используется для операций непредсказуемой продолжительности, таких как чтения файла или запись ответа HTTP. Поскольку продолжительность этих операций не является детерминированной, среда выполнения должна учитывать это для обеспечения эффективного использования ресурсов. Функция координирует горутины, потоки и процессоры, позволяя среде выполнения поддерживать производительность и отзывчивость во время блокирующих системных вызовов.

Тем не менее, не все системные вызовы являются непредсказуемыми. Например, получение идентификатора процесса или текущего времени обычно происходит быстро и стабильно. Для таких операций используется [RawSyscall](https://github.com/golang/go/blob/go1.24.0/src/syscall/syscall_linux.go#L54-L56) (от "raw" - прямой). Поскольку отсутствует планирование, связь между горутинами, потоками и процессорами остается неизменной при выполнении прямых системных вызовов.

На самом деле [Syscall](https://github.com/golang/go/blob/go1.24.0/src/syscall/syscall_linux.go#L72-L89) - это [RawSyscall](https://github.com/golang/go/blob/go1.24.0/src/syscall/syscall_linux.go#L54-L56) + дополнительная логика планирования.

```go
func Syscall(trap, a1, a2, a3 uintptr) (r1, r2 uintptr, err Errno) {
  runtime_entersyscall()
  r1, r2, err = RawSyscall6(trap, a1, a2, a3, 0, 0, 0)
  runtime_exitsyscall()
}
```

### Планирование в `Syscall`

Логика планирования реализована в функциях [runtime_entersyscall](https://github.com/golang/go/blob/go1.24.0/src/syscall/syscall_linux.go#L28-L29) и [runtime_exitsyscall](https://github.com/golang/go/blob/go1.24.0/src/syscall/syscall_linux.go#L31-L32). Эти функции соответствуют [runtime.entersyscall](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4512-L4532) и [runtime.exitsyscall](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4644-L4747). Связь создается при компиляции.

Перед выполнением системного вызова, среда выполнения записывает, что вызываемая горутина больше не использует ЦП. Горутина `G` переходит из состояния _running_ в состояние _syscall_, а ее указатель стека, счетчик программ, и указатель кадра сохраняются для последующего восстановления. Затем связь между потоком `M` и процессором `P` временно отключается, и `P` переходит в состояние _syscall_. Эта логика реализована в функции [runtime.reentersyscall](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4413-L4510), которая вызывается [runtime.entersyscall](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4512-L4532).

Что интересно, `sysmon` мониторит не только процессоры, выполняющие горутины (когда `P` находится в состоянии _running_), но также процессоры, выполняющие системные вызовы (когда `P` находится в состоянии _syscall_). Если `P` находится в состоянии _syscall_ дольше 10 мс, вместо некооперативного вытеснения выполняющейся горутины, происходит [передача управления процессором](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#processor-handoff-handoffp). Это сохраняет связь между горутиной `G` и потоком `M` и подключает к этому `P` другой поток `M1`, что позволяет выполнять готовые горутины в потоке `M1`. Поскольку `P` теперь выполняет код, его статус меняется с _syscall_ на _running_.

Обратите внимание, что пока системный вызов находится в процессе выполнения и независимо от того, блокируется (seize) `P` `sysmon` или нет, связь между горутиной `G` и потоком `M` сохраняется. Почему? Потому что программа Go (включая среду выполнения и пользовательский код) — это всего лишь процесс пользовательского пространства. Единственным средством выполнения, которое ядро ​​предоставляет процессу пользовательского пространства, является поток. Именно поток отвечает за выполнение кода среды выполнения, пользовательского кода и выполнение системных вызовов. Поток `M` выполняет системный вызов от имени какой-либо горутины `G`, поэтому связь между ними сохраняется. Следовательно, даже если `P` блокируется `sysmon`, `M` остается заблокированным, ожидая завершения системного вызова, прежде чем сможет вызвать функцию [runtime.exitsyscall](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4644-L4747).

Другой важный момент заключается в том, что пока процессор `P` находится в состоянии _syscall_, он не может быть взят другим потоком `M` для выполнения кода до тех пор, пока `sysmon` не заблокирует его или системный вызов не будет завершен. Следовательно, в случае одновременного выполнения нескольких системных вызовов программа (за исключением системных вызовов) не продвигается вперед. Именно поэтому база данных [Dgraph](https://docs.hypermode.com/dgraph/overview) жестко устанавливает [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) в значение `128`, чтобы ["позволить планировать больше вызовов дискового ввода-вывода"](https://github.com/hypermodeinc/dgraph/blob/v24.1.2/dgraph/main.go#L33-L36).

Как описано в [runtime.exitsyscall](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4644-L4747), существует два пути, которыми может пойти планировщик после завершения системного вызова: быстрый и медленный.

Быстрый путь возникает, если процессор может выполнить горутину `G`, только что завершившую системный вызов. Этим процессором может быть процессор, ранее выполнявший `G`, если он все еще находится в состоянии _syscall_ (т.е. не был заблокирован `sysmon`), или любой другой процессор в состоянии _idle_ в зависимости от того, какой найдется раньше. Обратите внимание, что после завершения системного вызова предыдущий процессор может больше не находиться в состоянии _syscall_, поскольку его заблокировал `sysmon`. Перед началом прохождения быстрого пути, `G` переходит из состояния _syscall_ в состояние _running_.

<img src="https://habrastorage.org/webt/w6/ho/jj/w6hojjtbybwrdpnv99hjzz4g3l0.png" />

_Быстрый путь системного вызова, когда `sysmon` не заблокировал процессор `P`_
<br />

<img src="https://habrastorage.org/webt/2x/si/i8/2xsii8vulhpvkszt7t8blf3sl8a.png" />

_Быстрый путь системного вызова, когда `sysmon` заблокировал процессор `P`_
<br />

В медленном пути планировщик еще раз пытается извлечь свободный процессор. Если такой обнаружен, горутина `G` планируется к выполнению на нем. Иначе, `G` помещается в глобальную очередь выполнения, и связанный поток `M` останавливается функцией [stopm](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#stop-thread-stopm) и ожидает пробуждения для продолжения [цикла планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop).

## Сетевой и файловый ввод-вывод

[Этот опрос](https://go.dev/blog/survey2024h2/what.svg) показывает, что 75% случаев использования Go — это разработка веб-сервисов, а 45% — статических сайтов. Это не совпадение, Go специально разработан для эффективного выполнения операций ввода-вывода, чтобы решить печально известную проблему [C10K](https://en.wikipedia.org/wiki/C10k_problem). Посмотрим, как Go обрабатывает операции ввода-вывода.

### HTTP-сервер изнутри

Создать HTTP-сервер на Go невероятно просто, например:

```go
package main

import "net/http"

func main() {
    http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
        w.WriteHeader(200)
    })

    http.ListenAndServe(":80", nil)
}
```

Функции, вроде `http.ListenAndServe` и `http.HandleFunc`, обманчиво просты, но на самом деле они абстрагируют большую сложность низкоуровневой работы с сетью. Go полагается на множество фундаментальных операций с [сокетами](https://en.wikipedia.org/wiki/Unix_domain_socket) для управления сетевой коммуникацией.

<img src="https://habrastorage.org/webt/o6/dd/sd/o6ddsd4dklvktq-riim1fpgksxw.png" />
<br />

В частности, `http.ListenAndServe()` использует системные вызовы [socket](https://man7.org/linux/man-pages/man2/socket.2.html), [bind](https://man7.org/linux/man-pages/man2/bind.2.html), [listen](https://man7.org/linux/man-pages/man2/listen.2.html) и [accept](https://man7.org/linux/man-pages/man2/accept.2.html) для создания TCP-сокетов, которые по сути являются [файловыми дескрипторами](https://en.wikipedia.org/wiki/File_descriptor). Этот метод привязывает TCP-сокет к указанному адресу и порту, "прослушивает" входящие соединения и создает новый подключенный сокет для обработки клиентских запросов. Это достигается без необходимости написания кода обработки сокетов. Аналогичным образом, `http.HandleFunc()` регистрирует пользовательские обработчики, абстрагируясь от низкоуровневых деталей, таких как использование системного вызова [read](https://man7.org/linux/man-pages/man2/read.2.html) для чтения данных и системного вызова [write](https://man7.org/linux/man-pages/man2/write.2.html) для записи данных в сетевой сокет.

<img src="https://habrastorage.org/webt/2x/tw/dp/2xtwdpf-rnq7cwqrlhhpfdciw4o.jpeg" />
<br />

Однако эффективно обрабатывать десятки тысяч одновременных запросов для HTTP-сервера не так-то просто. Go использует для этого несколько методов. Рассмотрим некоторые известные модели ввода-вывода в Linux и то, как Go использует их преимущества.

### Блокирующий ввод-вывод, неблокирующий ввод-вывод и мультиплексирование ввода-вывода

Операция ввода-вывода может быть блокирующей или неблокирующей. Когда поток вызывает блокирующий системный вызов, его выполнение приостанавливается до тех пор, пока вызов не завершится с запрошенными данными. В отличие от этого, неблокирующий ввод-вывод не приостанавливает поток; вместо этого он возвращает запрошенные данные, если они доступны, или ошибку ([EAGAIN](https://man7.org/linux/man-pages/man3/errno.3.html) или [EWOULDBLOCK](https://man7.org/linux/man-pages/man3/errno.3.html)), если данные еще не готовы. Блокирующий ввод-вывод проще в реализации, но неэффективен, поскольку требует от приложения создания N потоков для N соединений. В отличие от этого, неблокирующий ввод-вывод сложнее, но при правильной реализации обеспечивает значительно лучшее использование ресурсов. Визуальное сравнение этих двух моделей представлено на рисунках ниже.

<img src="https://habrastorage.org/webt/sq/2y/ji/sq2yjiwhk5a_sp8lihgnlfowopu.png" />

_Блокирующая модель ввода-вывода_
<br />

<img src="https://habrastorage.org/webt/fv/gm/zy/fvgmzyypkyrdg78x8ekfzxpi5hi.png" />

_Неблокирующая модель ввода-вывода_
<br />

Другая модель ввода-вывода, заслуживающая упоминания - это мультиплексирование ввода-вывода, в которой используется системный вызов [select](https://man7.org/linux/man-pages/man2/select.2.html) или [poll](https://man7.org/linux/man-pages/man2/poll.2.html) для ожидания готовности одного из набора файловых дескрипторов к выполнению ввода-вывода. В этой модели приложение блокируется при выполнении одного из этих системных вызовов, а не при выполнении настоящего системного вызова ввода-вывода, такого как [recvfrom](https://man7.org/linux/man-pages/man2/recv.2.html), показанного на рисунке выше. Когда [select](https://man7.org/linux/man-pages/man2/select.2.html) сообщает о готовности сокета к чтению, приложение вызывает [recvfrom](https://man7.org/linux/man-pages/man2/recv.2.html) для копирования запрошенных данных в буфер приложения в пользовательском пространстве.

<img src="https://habrastorage.org/webt/ip/ct/lf/ipctlfyltvomw2a5qdcytl5b8yw.png" />
<br />

### Модель ввода-вывода в Go

Go использует комбинацию неблокирующего ввода-вывода и мультиплексирования ввода-вывода для эффективной обработки соответствующих операций. Из-за ограничений производительности [select](https://man7.org/linux/man-pages/man2/select.2.html) и [poll](https://man7.org/linux/man-pages/man2/poll.2.html) (как объясняется в [этой статье](https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll/#why-don-t-we-use-poll-and-select)) Go избегает их в пользу более масштабируемых альтернатив: [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html) в Linux, [kqueue](https://man.freebsd.org/cgi/man.cgi?kqueue) в Darwin и [IOCP](https://learn.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports) в Windows. Go представляет `netpoll`, функцию, которая абстрагирует эти альтернативы для обеспечения единого интерфейса для мультиплексирования ввода-вывода в разных ОС.

## Работа `netpoll`

Работа `netpoll` включает 4 шага:

1. Создание экземпляра [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html) в пространстве ядра.
2. Регистрация в нем файловых дескрипторов.
3. Опрос через него файловых дескрипторов на предмет операций ввода-вывода.
4. Отмена регистрации файловых дескрипторов в экземпляре [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html).

### Создание экземпляра `epoll` и регистрация горутины

Когда обработчик TCP [принимает](https://github.com/golang/go/blob/go1.24.0/src/net/tcpsock.go#L374-L385) соединение, выполняется системный вызов [accept4](https://man7.org/linux/man-pages/man2/accept.2.html) с флагом [SOCK_NONBLOCK](https://man7.org/linux/man-pages/man2/socket.2.html#:~:text=of%0A%20%20%20%20%20%20%20socket()%3A-,SOCK_NONBLOCK,-Set%20the%20O_NONBLOCK) для установки файлового дескриптора сокета в неблокирующий режим. После этого создается несколько дескрипторов для интеграции с `netpoll` среды выполнения Go.

1. Создается экземпляр [new.netFD](https://github.com/golang/go/blob/go1.24.0/src/net/fd_posix.go#L16-L27) для оборачивания файлового дескриптора сокета. Эта структура предоставляет высокоуровневую абстракцию для выполнения сетевых операций на нижележащем файловом дескрипторе ядра. После инициализации экземпляра [net.netFD](https://github.com/golang/go/blob/go1.24.0/src/net/fd_posix.go#L16-L27), выполняется системный вызов [epoll_create](https://man7.org/linux/man-pages/man2/epoll_create.2.html) для создания экземпляра [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html). Он инициализируется в функции [poll_runtime_pollServerInit](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L213-L216), которая обернута в [sync.Once](https://pkg.go.dev/sync#Once) для обеспечения однократного выполнения. Благодаря [sync.Once](https://pkg.go.dev/sync#Once) внутри процесса Go существует только один экземпляр [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html), который используется на протяжении всего жизненного цикла процесса.
2. Внутри [poll_runtime_pollOpen](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L243-L278) среда выполнения Go выделяет экземпляр [runtime.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L72-L115), содержащий метаданные планирования и [ссылки](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L98-L101) на горутины, вовлеченные в ввод-вывод. Затем файловый дескриптор сокета регистрируется в списке интересов (interest list) [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html) с помощью системного вызова [epoll_ctl](https://man7.org/linux/man-pages/man2/epoll_ctl.2.html) с операцией [EPOLL_CTL_ADD](https://man7.org/linux/man-pages/man2/epoll_ctl.2.html#:~:text=op%20argument%20are%3A-,EPOLL_CTL_ADD,-Add%20an%20entry). Поскольку [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html) отслеживает файловые дескрипторы, а не горутины, [epoll_ctl](https://man7.org/linux/man-pages/man2/epoll_ctl.2.html) также связывает файловый дескриптор с экземпляром [runtime.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L72-L115), что позволяет планировщику определить, какую горутину следует возобновить при получении сообщения о готовности к операциям ввода-вывода.
3. Создается экземпляр [poll.FD](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_unix.go#L17-L48) для управления операциями чтения и записи с поддержкой опроса. Он содежит ссылку на [runtime.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L72-L115) через [poll.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_poll_runtime.go#L32-L34) (которая является просто оберткой).

> В Go есть проблема с использованием одного экземпляра `epoll`, как описано в [этом открытом вопросе](https://github.com/golang/go/issues/65064). Ведутся дискуссии о том, [следует ли Go использовать один или несколько экземпляров epoll](https://github.com/golang/go/issues/65064#issuecomment-1896633168), или даже [другую модель мультиплексирования ввода-вывода, например, `io_uring`](https://github.com/golang/go/issues/31908).

Из-за успешности этой модели для сетевого ввода-вывода, Go также использует [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html) для файлового ввода-вывода. После открытия файла, вызывается функция [syscall.SetNonblock](https://github.com/golang/go/blob/go1.24.0/src/os/file_unix.go#L222-L222) для включения неблокирующего режима для файлового дескриптора. Затем инициализируются экземпляры [poll.FD](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_unix.go#L17-L48), [poll.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_poll_runtime.go#L32-L34) и [runtime.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L72-L115) для регистрации файлового дескриптора в списке интересов [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html), позволяя файловому дескриптору быть мультиплексированным.

Отношения между этими дескрипторами изображены на рисунке ниже. В то время как [net.netFD](https://github.com/golang/go/blob/go1.24.0/src/net/fd_posix.go#L16-L27), [os.File](https://github.com/golang/go/blob/go1.24.0/src/os/types.go#L15-L20), [poll.FD](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_unix.go#L17-L48) и [poll.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_poll_runtime.go#L32-L34) реализованы в пользовательском коде Go (в стандартной библиотеке), [runtime.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L72-L115) находится в самой среде выполнения.

<img src="https://habrastorage.org/webt/cj/o3/dh/cjo3dh9qnb5rhktzhalwf1w1nq4.png" />
<br />

### Опрос файловых дескрипторов

Когда горутина читает из сокета или файла, она в конечном счете вызывает метод [Read](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_unix.go#L141-L173) [poll.FD](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_unix.go#L17-L48). В этом методе горутина выполняет системный вызов [read](https://man7.org/linux/man-pages/man2/read.2.html) для получения любых доступных данных из дескриптора файла. Если данные ввода-вывода еще не готовы, т.е. вернулась ошибка `EAGAIN`, среда выполнения вызывает метод [poll_runtime_pollWait](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L336-L361) для [парковки горутины](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#goroutine-parking-gopark). Тоже самое происходит, когда горутина пишет в сокет или файл, за исключением того, что [Read](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_unix.go#L141-L173) заменяется на [Write](https://github.com/golang/go/blob/go1.24.0/src/net/net.go#L201-L211), а системный вызов [read](https://man7.org/linux/man-pages/man2/read.2.html) - на [write](https://man7.org/linux/man-pages/man2/write.2.html). Теперь горутина находится в состоянии _waiting_. Когда ее файловый дескриптор готов к вводу-выводу, `netpoll` передает ее среде выполнения для возобновления.

В среде выполнения Go `netpoll` - это всего лишь функция с таким же названием. В функции [netpoll](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll_epoll.go#L91-L176) используется системный вызов [epoll_wait](https://man7.org/linux/man-pages/man2/epoll_wait.2.html) для мониторинга до `128` файловых дескрипторов за определенный промежуток времени. Этот системный вызов возвращает экземпляры [runtime.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L72-L115), которые были зарегистрированы ранее (как описано в предыдущем разделе) для каждого файлового дескриптора, ставшего готовым. Наконец, `netpoll` извлекает ссылки горутин из [runtime.pollDesc](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L72-L115) и передает их среде выполнения.

Но когда именно функция `netpoll` вызывается? Она запускается, когда поток ищет готовую к выполнению горутину, как указано в [цикле планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop). Согласно функции [findRunnable](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3267-L3646), среда выполнения Go обращается к `netpoll` только в том случае, если в локальной очереди выполнения текущего `P` или в глобальной очереди выполнения нет доступных горутин. Это означает, что даже если дескриптор файла готов к вводу-выводу, горутина не обязательно будет немедленно разбужена.

Как упоминалось ранее, `netpoll` может блокироваться на определенное время, которое определяется параметром `delay`. Если значение `delay` положительное, блокировка длится указанное количество наносекунд. Если значение `delay` отрицательное, блокировка продолжается до тех пор, пока не будет готово событие ввода-вывода. В противном случае, если `delay` равняется `0`, функция немедленно возвращает все события ввода-вывода, которые в данный момент готовы. В функции [findRunnable](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3267-L3646) параметр `delay` передается со значением `0`. Это означает, что если одна горутина ожидает ввода-вывода, другая горутина может быть запланирована для выполнения в том же потоке ядра.

### Отмена регистрации файловых дескрипторов

Как упоминалось выше, экземпляр [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html) отслеживает до `128` файловых дескрипторов. Поэтому важно отменять регистрацию файловых дескрипторов, когда они больше не нужны, иначе некоторые горутины могут оказаться в состоянии "голодания". Когда файловое или сетевое соединение больше не используется, его следует закрыть, вызвав соответствующий метод `Close`.

Внутри вызывается метод [destroy](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_unix.go#L75-L87) [poll.FD](https://github.com/golang/go/blob/go1.24.0/src/internal/poll/fd_unix.go#L75-L87). Этот метод в конечном итоге вызывает функцию [poll_runtime_pollClose](https://github.com/golang/go/blob/go1.24.0/src/runtime/netpoll.go#L280-L295) в среде выполнения для выполнения [epoll_ctl](https://man7.org/linux/man-pages/man2/epoll_ctl.2.html) с операцией `EPOLL_CTL_DEL`. Это отменяет регистрацию файлового дескриптора в списке интересов [epoll](https://man7.org/linux/man-pages/man7/epoll.7.html).

### Резюме

На рисунке ниже показан весь процесс работы `netpoll` в среде выполнения Go с файловым вводом-выводом. Процесс для сетевого ввода-вывода аналогичен, но с добавлением обработчика TCP, принимающего и закрывающего соединения. Для простоты другие компоненты, такие как `sysmon` и прочие процессоры в состоянии _idle_, опущены.

<img src="https://habrastorage.org/webt/bd/ad/p1/bdadp1q7me2w6_bndgbihl_q4l8.png" />
<br />

## Сборщик мусора

Возможно, вы знаете, что в Go имеется сборщик мусора (garbage collector, GC) для автоматического освобождения памяти от неиспользуемых объектов. Однако, как упоминалось в разделе "Загрузка программы", при запуске программы изначально нет потоков для выполнения сборщика мусора. Так где же он на самом деле работает?

Прежде чем ответить на этот вопрос, давайте кратко рассмотрим, как работает сборка мусора. Go использует трассирующий (tracing) сборщик мусора, который идентифицирует активные/живые (live) и неактивные/мертвые (dead) объекты, обходя выделенный граф объектов, начиная с набора корневых ссылок (root references). Объекты, достижимые из корневых ссылок, считаются активными, недостижимые - неактивными и подлежат освобождению ресурсов.

Сборщик мусора использует [трехцветный алгоритм маркировки](https://en.wikipedia.org/wiki/Tracing_garbage_collection#Tri-color_marking) с поддержкой [слабых ссылок](https://learn.microsoft.com/en-us/dotnet/standard/garbage-collection/weak-references). Такая конструкция позволяет сборщику мусора работать параллельно с программой, значительно сокращая паузы, приводящие к остановке выполнения программы ("остановке мира") (stop-the-world, STW), и повышая общую производительность.

Цикл сборки мусора можно разделить на 4 этапа:

1. Первая STW: процесс приостанавливается, чтобы все процессоры могли перейти в безопасную точку (safe point).
2. Маркировка: горутины GC ненадолго "отвлекают" `P` для маркировки достижимых объектов.
3. Вторая STW: процесс снова приостанавливается, чтобы GC мог финализировать стадию маркировки.
4. Очистка: процесс возобновляется, память для недоступных объектов освобождается в фоновом режиме.

Обратите внимание, что на шаге 2 горутина сборщика мусора выполняется параллельно с обычными горутинами на том же процессоре `P`. Функция [findRunnable](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3267-L3646) ищет не только обычные горутины, но также горутины GC (шаги 1 и 2).

## Общие функции

### Получение горутины: [getg](https://github.com/golang/go/blob/go1.24.0/src/runtime/stubs.go#L28-L31)

В среде выполнения Go имеется общая функция, которая используется для извлечения выполняющейся в текущем потоке ядра горутины - [getg](https://github.com/golang/go/blob/go1.24.0/src/runtime/stubs.go#L28-L31). Вы не найдете реализацию этой функции в исходном коде. Это связано с тем, что при компиляции компилятор переписывает ее вызовы в инструкции, которые извлекают горутину из [локального хранилища потоков](https://en.wikipedia.org/wiki/Thread-local_storage) (TLS) или из регистров.

Но когда текущая горутина сохраняется в TLS, чтобы ее можно было извлечь позже? Это происходит при переключении контекста горутины в функции [gogo](https://github.com/golang/go/blob/go1.24.0/src/runtime/asm_amd64.s#L411-L413), которая вызывается [execute](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3221-L3265). Это также происходит при вызове обработчика сигналов в функции [sigtrampgo](https://github.com/golang/go/blob/go1.24.0/src/runtime/signal_unix.go#L420-L495).

### Парковка горутины: [gopark](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L390-L436)

Это широко используемая процедура в среде выполнения Go для перевода текущей горутины в состояние _waiting_ и планирования выполнения другой горутины. Приведенный ниже фрагмент кода демонстрирует некоторые из ее ключевых аспектов.

```go
func gopark(unlockf func(*g, unsafe.Pointer) bool, ...) {
    ...
    mp.waitunlockf = unlockf
    ...
    releasem(mp)
    ...
    mcall(park_m)
}
```

Внутри функции [releasem](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime1.go#L612-L619) [stackguard0](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L405-L405) горутины устанавливается в [stackPreempt](https://github.com/golang/go/blob/go1.24.0/src/runtime/stack.go#L128-L130) для запуска последующего кооперативного вытеснения. Управление затем передается системной горутине [g0](https://github.com/golang/go/blob/go1.24.0/src/runtime/runtime2.go#L529), которая принадлежит тому же потоку, на котором выполняется горутина, для вызова функции [park_m](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4089-L4142).

В [park_m](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4089-L4142) состояние горутины устаналивается в _waiting_ и связь между горутиной и потоком `M` уничтожается. Кроме того, [gopark](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L390-L436) получает функцию обратного вызова `unlockf`, которая выполняется в [park_m](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L4089-L4142). Если `unlockf` возвращает `false`, припаркованная горутина немедленно запускается снова и повторно планируется на том же потоке с помощью [execute](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3221-L3265). Иначе, `M` входит в [цикл планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop) для поиска и выполнения горутины.

### Запуск потока: [startm](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L2917-L3025)

Это функция отвечает за планирование потока `M` для запуска определенного процессора `P`. Рисунок ниже показывает процесс выполнения этой функции, в котором поток `M1` является предком потока `M2`.

<img src="https://habrastorage.org/webt/ch/mi/2d/chmi2d3pf8wtpzg8kvk5x8aou8c.png" />
<br />

Если `P` равняется `nil`, она пытается извлечь свободный (idle) процессор из глобального списка. Если свободного процессора нет, функция просто возвращается. Это указывает на то, что используется максимальное количество процессоров и дополнительный поток не может быть создан или возобновлен. Если свободный процессор найден (или `P` уже был предоставлен), функция либо создает новый поток `M1` или будит существующий для запуска `P`.

После пробуждения, существующий поток `M` продолжает выполнение [цикла планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop). Новый поток создается через системный вызов [clone](https://man7.org/linux/man-pages/man2/clone.2.html) с [mstart](https://github.com/golang/go/blob/go1.24.0/src/runtime/os_linux.go#L186-L187) в качестве входной точки. Функция [mstart](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L1769-L1771) входит в [цикл планирования](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#schedule-loop), где ищет готовую к выполнению горутину.

### Остановка потока: [stopm](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L2889-L2910)

Эта функция добавляет поток `M` в список свободных и усыпляет его. [stopm](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L2889-L2910) не возвращается, пока `M` не будет разбужен, как правило, при создании новой горутины. Это достигается с помощью системного вызова [futex](https://linux.die.net/man/2/futex), благодаря чему `M` не потребляет ресурсы процессора в период ожидания.

### Передача управления процессором: [handoff](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3026-L3096)

[handoff](https://github.com/golang/go/blob/go1.24.0/src/runtime/proc.go#L3026-L3096) отвечает за передачу процессора `P` от потока `M`, заблокированного в системном вызове, потоку `M1`. `P` будет связан с `M1` для дальнейшего выполнения путем вызова [startm](https://nghiant3223.github.io/2025/04/15/go-scheduler.html#start-thread-startm) при определенных условиях: если глобальная очередь выполнения не пуста, если локальная очередь выполнения не пуста, если есть работа по трассировке или сборке мусора или если в данный момент ни один поток не обрабатывает `netpoll`. Если ни одно из этих условий не выполняется, `P` возвращается в список свободных процессоров.

## API среды выполнения Go

Среда выполнения предоставляет несколько API для взаимодействия с планировщиком и горутинами. Она также позволяет программистам настраивать планировщик и другие колмпоненты, такие как сборщик мусора, для нужд конкретного приложения.

### [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS)

Эта функция устанавливает количество процессоров в среде выполнения, что определяет уровень параллелизма программы. Дефолтным значением [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) является значение функции [runtime.NumCPU](https://pkg.go.dev/runtime#NumCPU), которая запрашивает у ОС информацию о выделенных процессорах для процесса Go.

Дефолтное значение [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) может быть проблематичным, особенно в контейнеризованных средах, как описано в [этом замечательном посте](https://dev.to/rdforte/the-implications-of-running-go-in-a-containerised-environment-3bp1). Сейчас обсуждается предложение о том, чтобы функция [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) учитывала ограничения квот (quota limits) cgroup для ЦП, что призвано улучшить ее поведение в таких средах. В будущих версиях Go функция [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) может быть признана устаревшей, как отмечено в официальной документации: ["Эта функция будет удалена после улучшения планировщика"](https://github.com/golang/go/blob/3901409b5d0fb7c85a3e6730a59943cc93b2835c/src/runtime/debug.go#L15-L15).

Некоторым программам, интенсивно использующим ввод-вывод, может быть полезно иметь большее количество процессоров, чем предусмотрено по умолчанию. Например, в базе данных [Dgraph](https://github.com/hypermodeinc/dgraph/blob/v24.1.2/dgraph/main.go#L36) значение [GOMAXPROCS](https://pkg.go.dev/runtime#GOMAXPROCS) жестко задано равным `128`, что позволяет планировать больше операций ввода-вывода.

### [Goexit](https://pkg.go.dev/runtime#Goexit)

Эта функция корректно завершает текущую горутину. Все отложенные вызовы выполняются до завершения горутины. Программа продолжает выполнение других горутин. Если все остальные горутины завершаются (exit), программа падает (crashe). [Goexit](https://pkg.go.dev/runtime#Goexit) следует использовать в тестировании, а не в реальных приложениях, когда необходимо прервать тестовый случай досрочно (например, если не выполнены предварительные условия), но при этом требуется выполнение отложенной очистки.

## Заключение

Планировщик задач Go — это мощная и эффективная система, обеспечивающая легковесную параллельность с помощью горутин. В этой статье мы рассмотрели его эволюцию от примитивной модели до архитектуры GMP и ключевые компоненты, такие как создание горутин, вытеснение, обработка системных вызовов и интеграция с `netpoll`.

Надеюсь, эти знания позволят вам писать более эффективные и надежные программы на Go.

## Ссылки

- [kelche.co. Go Scheduling](https://www.kelche.co/blog/go/golang-scheduling)
- [unskilled.blog. Preemption in Go](https://unskilled.blog/posts/preemption-in-go-an-introduction/)
- [Ian Lance Taylor. What is system stack?](https://groups.google.com/g/golang-nuts/c/JCKWH8fap9o)
- [Michael Kerrisk. The Linux Programming Interface](https://man7.org/tlpi/)
- [W. Richard Stevens. Unix Network Programming](https://www.amazon.com/UNIX-Network-Programming-Richard-Stevens/dp/0139498761)
- [zhuanlan.zhihu.com. Golang program startup process analysis](https://zhuanlan.zhihu.com/p/436925356)
- [Madhav Jivrajani. GopherCon 2021: Queues, Fairness, and The Go Scheduler](https://www.youtube.com/watch?v=wQpC99Xu1U4&t=2375s&ab_channel=GopherAcademy)
- [Abraham Silberschatz, Peter B. Galvin, Greg Gagne. Operating System Concepts](https://www.amazon.com/Operating-System-Concepts-Abraham-Silberschatz/dp/1119800366/ref=zg-te-pba_d_sccl_3_1/138-7692107-2007040)
